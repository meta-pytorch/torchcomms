// Copyright (c) Meta Platforms, Inc. and affiliates.

#pragma once

#include <cstddef>
#include <sstream>
#include "comms/ctran/algos/AllToAllvDedup/FwdGroupSync.h"
#include "comms/ctran/algos/AllToAllvDedup/WorkerSync.h"
#include "comms/ctran/algos/CtranAlgoDev.h"
#include "comms/ctran/algos/common/GpeKernelSync.h"
#include "comms/ctran/algos/common/MPSCTbSync.h"
#include "comms/utils/commSpecs.h" // need for ncclDataType_t

// Define types shared by both host and device code

namespace ctran::alltoallvdedup {

// Prepare kernel leverages warp for different duty; use them to control
// required threads
enum class PrepareKernRole {
  kCompOffset,
  kCopyNumRecvBlocks,
  kCopyNumForwardBlocks,
  kCopyNumSendBlocks,
  kResetSync,
  kNumRoles
};

enum class PrepareSyncStep {
  kCopyBlockRecvBuckets,
  kPostTmpNumSendBlocksBuff,
  kCopyLocalOutputSplits,
  kPostNumForwardBlocks,
  kCopyNumRecvBlocksH,
  kKernelDone,
};

struct PersistConfig {
  int numPrepareThreadBlocks;
  int numPrepareThreads;

  int numThreadBlocks;
  int numThreads;

  int numSendGroups;
  int numSendWorkers;

  int numFwdGroups;
  int numFwdWorkers;

  int numRecvGroups;
  int numRecvWorkers;

  int tmpChunkSize;
  int tmpNumChunks;
};

// Persistent arguments specified at init time
struct PersistArgs {
  size_t totalNumSendBlocks;
  size_t blockCount;
  size_t blockNumRecvBuckets;
  int numRecvBuckets;
  commDataType_t datatype;
};

struct PrepareArgs {
  const int* blockRecvBuckets;
  size_t* numSendBlocks;
  size_t* numRecvBlocks;
  size_t* recvOffsets;
  size_t* numForwardBlocks;
  size_t* totalNumRecvBlocks;
  int* xnodeInputSplits;
  int* xnodeOutputSplits;
  int* xnodeGatherIndices;
  int* localInputSplits;
  int* localOutputSplits;
  int* localGatherIndices;
  int* eGatherIndices;
};

struct ExecArgs {
  // Dispatch input arguments
  const void* sendBuff;
  const int* blockRecvBuckets;
  const size_t* numSendBlocks;
  const size_t* numRecvBlocks;
  const size_t* recvOffsets;
  const size_t* numForwardBlocks;
  const int* sendIdx;
  const int* fwdIdx;
  const int* recvIdx;

  // Dispatch output arguments
  void* recvBuff;
  int* blockSendRanks;
};

struct KernSync {
  // used to sync between kernel and GPE on forward rank at prepare phase.
  algos::GpeKernelSync* prepareGKSync;

  // used to sync between forward groups in the same rank
  FwdGroupSync* fwdGroupSync;

  // used to sync between workers in the same group
  WorkerSync* workerSync;

  // used to sync between forward groups and receive local ranks' recv groups on
  // the same node
  algos::MPSCTbSync<>* fwdRecvSyncs;
  algos::MPSCTbSync<>* remFwdRecvSyncs[CTRAN_MAX_NVL_PEERS];

  // used to sync between kernel and GPE on send rank and forward rank,
  // respectively. Do not use arguments in kElem since it is slow host-pinned
  // memory. Each element in the list is for one rail peer.
  algos::GpeKernelSync* sendCopyGKSyncs;
  algos::GpeKernelSync* recvFwdGKSyncs;
  algos::GpeKernelSync* recvCopyGKSyncs;
};

struct InitKernArgs {
  PersistArgs pArgs;
  PersistConfig config;
  KernSync kSync;
};

struct PrepareKernArgs {
  uint64_t opCount;
  PersistConfig config;

  PersistArgs pArgs;
  PrepareArgs prepareArgs;

  int* blockRecvBucketsH;
  size_t* numForwardBlocksH;

  size_t* tmpNumRecvBlocksBuff;
  size_t* tmpNumRecvBlocksBuffH;

  // IPC imported ptr to each of the local peers' tmpNumRecvBlocksBuff
  size_t* tmpRemNumRecvBlocksBuffs[CTRAN_MAX_NVL_PEERS];
  size_t* tmpNumSendBlocksBuffH;

  int* tmpLocalOutputSplits;
  // IPC imported ptr to each of the local peers' tmpLocalOutputSplits
  int* tmpRemLocalOutputSplits[CTRAN_MAX_NVL_PEERS];
  int* tmpLocalOutputSplitsH;
  int* tmpRankBitmaps;
  // IPC imported ptr to each of the local peers' tmpRankBitmaps
  int* tmpRemRankBitmaps[CTRAN_MAX_NVL_PEERS];
  int* tmpRankBitmapsH;

  // Copy of recvOffsets used by recv rank to track the offset of
  // incoming blocks in exec.
  size_t* tmpRecvOffsets;

  KernSync kSync;
};

struct ExecKernArgs {
  uint64_t opCount;
  PersistArgs pArgs;
  ExecArgs execArgs;
  PersistConfig config;

  // Used by send rank to copy blocks into contig fixed size chunk for RDMA
  void* tmpSendBuff;
  // used by fwd rank to load incoming fwdArgs + data chunk
  void* tmpFwdBuff;

  void* tmpSendIdx;
  void* tmpIntraFwdIdx;

  void* tmpRecvBuff;
  // IPC imported ptr to each of the local peers' tmpRecvBuff
  void* remTmpRecvBuffs[CTRAN_MAX_NVL_PEERS];

  // Copy of recvOffsets copied in prepare, for recv rank to track the offset of
  // incoming blocks in exec.
  size_t* tmpRecvOffsets;

  KernSync kSync;
};

// FWD metadata generated by sender side, and transfer to fwd rank's
// tmpbuf in GPU mem together with data chunk.
// Format: fwdHdr + recvLocalRanksBitMap[numBlocks] + blocks[numBlocks]
struct FwdChkHdr {
  int numBlocks; // number of actual blocks in the fwd chunk
  int sendRank; // sender rank of all chunks in the fwd chunk
};

// FIXME: recvRanksBitMap is a 32 bits integer, each bit represents a
// recvRank, can hold up to 32 localRanks. It is sufficient for H100, but not
// for GB200!
struct LocalBucketsBitMap {
  uint32_t val;
  DEVICE_ATTRIBUTE void insert(const int i) {
    val |= 1 << i;
  }

  DEVICE_ATTRIBUTE bool contains(const int i) {
    return val & (1 << i);
  }

  // Check if any of the bits in [min, max] are set
  DEVICE_ATTRIBUTE bool containsAny(const int min, const int max) {
    for (int i = min; i <= max; i++) {
      if (contains(i)) {
        return true;
      }
    }
    return false;
  }
};

// MyRankBucketsBitMap holds the recvBuckets for the localRank
// It is a subset of the LocalBucketsBitMap containing only information for
// localRank
struct MyRankBucketsBitMap {
  uint32_t val;

  DEVICE_ATTRIBUTE MyRankBucketsBitMap(
      LocalBucketsBitMap localBitMap,
      int localRank,
      int numRecvBuckets) {
    uint32_t mask = (1 << numRecvBuckets) - 1;
    val = (localBitMap.val >> (localRank * numRecvBuckets)) & mask;
  }

  DEVICE_ATTRIBUTE bool contains(const int i) {
    return val & (1 << i);
  }
};
} // namespace ctran::alltoallvdedup
