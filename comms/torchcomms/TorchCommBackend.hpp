// Copyright (c) Meta Platforms, Inc. and affiliates.

#pragma once

#include <ATen/ATen.h>
#include <c10/core/Device.h>
#include <c10/util/intrusive_ptr.h>
#include <comms/torchcomms/TorchCommBatch.hpp>
#include <comms/torchcomms/TorchCommLogging.hpp>
#include <comms/torchcomms/TorchCommOptions.hpp>
#include <comms/torchcomms/TorchCommTypes.hpp>
#include <comms/torchcomms/TorchCommUtils.hpp>
#include <comms/torchcomms/TorchCommWindow.hpp>
#include <comms/torchcomms/TorchWork.hpp>
#include <torch/csrc/distributed/c10d/Store.hpp> // @manual=//caffe2:torch-cpp-cpu
#include <torch/csrc/distributed/c10d/Work.hpp> // @manual=//caffe2:torch-cpp-cpu
#include <memory>
#include <vector>

namespace torch {
namespace comms {

inline const char* TORCHCOMM_BACKEND_ABI_VERSION = "1.0";

class TorchCommBackend {
 public:
  virtual ~TorchCommBackend() = default;

  // Initialize the communication backend
  virtual void init(
      at::Device device,
      const std::string& name,
      const CommOptions& options = {}) = 0;
  virtual void finalize() = 0;
  virtual int getRank() const = 0;
  virtual int getSize() const = 0;

  // Name of the backend impl that's the same for all instances of a backend.
  virtual std::string_view getBackendName() const = 0;
  // Unique name for this instance of the communicator.
  virtual std::string_view getCommName() const = 0;

  // Point-to-Point Operations
  virtual std::shared_ptr<TorchWork> send(
      const at::Tensor& tensor,
      int dst,
      bool async_op,
      const SendOptions& options = {}) = 0;
  virtual std::shared_ptr<TorchWork> recv(
      at::Tensor& tensor,
      int src,
      bool async_op,
      const RecvOptions& options = {}) = 0;

  virtual std::shared_ptr<TorchWork> batch_op_issue(
      const std::vector<BatchSendRecv::P2POp>& ops,
      bool async_op,
      const BatchP2POptions& options = {}) = 0;

  // Collective Operations
  virtual std::shared_ptr<TorchWork> broadcast(
      at::Tensor& tensor,
      int root,
      bool async_op,
      const BroadcastOptions& options = {}) = 0;
  virtual std::shared_ptr<TorchWork> all_reduce(
      at::Tensor& tensor,
      ReduceOp op,
      bool async_op,
      const AllReduceOptions& options = {}) = 0;
  virtual std::shared_ptr<TorchWork> reduce(
      const at::Tensor& tensor,
      int root,
      ReduceOp op,
      bool async_op,
      const ReduceOptions& options = {}) = 0;
  virtual std::shared_ptr<TorchWork> all_gather(
      const std::vector<at::Tensor>& tensor_list,
      const at::Tensor& tensor,
      bool async_op,
      const AllGatherOptions& options = {}) = 0;
  virtual std::shared_ptr<TorchWork> all_gather_single(
      at::Tensor& output,
      const at::Tensor& input,
      bool async_op,
      const AllGatherSingleOptions& options = {}) = 0;
  virtual std::shared_ptr<TorchWork> reduce_scatter(
      at::Tensor& output,
      const std::vector<at::Tensor>& input_list,
      ReduceOp op,
      bool async_op,
      const ReduceScatterOptions& options = {}) = 0;
  virtual std::shared_ptr<TorchWork> reduce_scatter_single(
      at::Tensor& output,
      const at::Tensor& input,
      ReduceOp op,
      bool async_op,
      const ReduceScatterSingleOptions& options = {}) = 0;
  virtual std::shared_ptr<TorchWork> all_to_all_single(
      at::Tensor& output,
      const at::Tensor& input,
      bool async_op,
      const AllToAllSingleOptions& options = {}) = 0;
  virtual std::shared_ptr<TorchWork> all_to_all_v_single(
      at::Tensor& output,
      const at::Tensor& input,
      const std::vector<uint64_t>& output_split_sizes,
      const std::vector<uint64_t>& input_split_sizes,
      bool async_op,
      const AllToAllvSingleOptions& options = {}) = 0;
  virtual std::shared_ptr<TorchWork> all_to_all(
      const std::vector<at::Tensor>& output_tensor_list,
      const std::vector<at::Tensor>& input_tensor_list,
      bool async_op,
      const AllToAllOptions& options = {}) = 0;
  virtual std::shared_ptr<TorchWork> barrier(
      bool async_op,
      const BarrierOptions& options = {}) = 0;

  // Scatter and Gather Operations
  virtual std::shared_ptr<TorchWork> scatter(
      at::Tensor& output_tensor,
      const std::vector<at::Tensor>& input_tensor_list,
      int root,
      bool async_op,
      const ScatterOptions& options = {}) = 0;
  virtual std::shared_ptr<TorchWork> gather(
      const std::vector<at::Tensor>& output_tensor_list,
      const at::Tensor& input_tensor,
      int root,
      bool async_op,
      const GatherOptions& options = {}) = 0;

  // Communicator Management
  virtual std::shared_ptr<TorchCommBackend> split(
      const std::vector<int>& ranks,
      const std::string& name,
      const CommOptions& options = {}) = 0;

  virtual const CommOptions& getOptions() const = 0;

  virtual const at::Device& getDevice() const = 0;
  // Window & One-sidede Operations, not required for all backends, so we added
  // default implementation here
  virtual std::shared_ptr<TorchCommWindow> window_allocate(
      const size_t window_size,
      bool cpu_buf = false,
      const size_t signalSize = 256) {
    (void)(window_size);
    (void)(cpu_buf);
    (void)(signalSize);
    throw std::logic_error(
        "[TorchCommBackend]: window_allocate not implemented for communicator:" +
        std::string(getCommName()));
    return nullptr;
  }
};

/**
 * Interface for a dynamic loader to be able to load a backend library
 * from a dynamic library.
 */
struct DynamicLoaderInterface {
  // Function pointers
  TorchCommBackend* (*new_comm)(void);
  void (*destroy_comm)(TorchCommBackend* comm);
  const char* (*get_supported_version)();
};

// Factory function signature (implemented in each .so)
using CreateDynamicLoaderFn = DynamicLoaderInterface (*)();

} // namespace comms
} // namespace torch
