
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Getting Started &#8212; meta-pytorch/torchcomms main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=047068a3" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css?v=e14e8605" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css?v=e14e8605" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=a8da1a53"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'getting_started';</script>
    <link rel="canonical" href="https://meta-pytorch.org/torchcomms/getting_started.html" />
    <link rel="icon" href="_static/torchcomms-logo-favicon.png"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="API Reference" href="api.html" />
    <link rel="prev" title="torchcomms" href="index.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<style>
  :root {
    --header-height: 0px !important;
    --header-height-desktop: 0px !important;
  }
</style>


<style>
  @media (min-width: 1100px) {
    .site-footer {
      height: 300px !important;
    }
  }
</style>

<link rel="stylesheet" type="text/css" href="_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="_static/js/theme.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="https://docs.pytorch.org/docs/stable/_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="torchcomms">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NPLPKN5G" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-NPLPKN5G');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'v0.0.1');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Repository configuration for tutorials -->

<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>

<!--
   Search engines should not index the main version of documentation.
   Stable documentation are built without release == 'main'.
   -->
<meta name="robots" content="noindex">


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>

<body data-feedback-url="https://github.com/meta-pytorch/torchcomms" class="pytorch-body">
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo-light.png" class="logo__image only-light" alt="meta-pytorch/torchcomms main documentation - Home"/>
    <script>document.write(`<img src="_static/logo-dark.png" class="logo__image only-dark" alt="meta-pytorch/torchcomms main documentation - Home"/>`);</script>
  
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item current active">
  <a class="nav-link nav-internal" href="#">
    Getting Started
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="api.html">
    API Reference
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/meta-pytorch/torchcomms" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://dev-discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item current active">
  <a class="nav-link nav-internal" href="#">
    Getting Started
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="api.html">
    API Reference
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/meta-pytorch/torchcomms" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://dev-discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"></div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page">Getting Started</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="Getting Started">
        <meta itemprop="position" content="1">
      </div>
    </div>

    
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="getting-started">
<h1>Getting Started<a class="headerlink" href="#getting-started" title="Link to this heading">#</a></h1>
<p>torchcomms is an experimental, lightweight communication API for
<a class="reference external" href="https://docs.pytorch.org/docs/stable/distributed.html">PyTorchDistributed(PTD)</a>.
It provides a simplified, object-oriented interface
for distributed collective operations and offers both high-level
collective APIs and multiple out-of-the-box backends.</p>
<p>torchcomms provides:</p>
<ul class="simple">
<li><p><strong>Simplified Object-Oriented API</strong>: A clean, intuitive interface for communication operations</p></li>
<li><p><strong>Support for Multiple Backends</strong>, including:</p>
<ul>
<li><p><strong>NCCLX</strong>: NVIDIA Collective Communications Library (extended) - Meta’s production-tested backend that powers all generative AI services</p></li>
<li><p><strong>NCCL</strong>: Standard NCCL backend for NVIDIA GPUs</p></li>
<li><p><strong>GLOO</strong>: CPU-based backend for CPU tensors and metadata transfer</p></li>
<li><p><strong>RCCL</strong>: AMD ROCm Collective Communications Library for AMD GPUs</p></li>
</ul>
</li>
<li><p><strong>Synchronous and Asynchronous Operations</strong>: Flexible execution modes for different performance needs</p></li>
<li><p><strong>Native PyTorch Integration</strong>: Works seamlessly with PyTorch tensors and CUDA streams</p></li>
<li><p><strong>Scalable</strong>: Designed to scale to 100,000+ GPUs</p></li>
</ul>
<p>Common use cases for torchcomms include distributed training of neural networks,
multi-GPU data parallelism, model parallelism across multiple devices,
and collective communication patterns such as AllReduce, Broadcast,
Send/Recv, and other operations.</p>
<section id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Link to this heading">#</a></h2>
<p>torchcomms requires the following software and hardware:</p>
<ul class="simple">
<li><p>Python 3.10 or higher</p></li>
<li><p>PyTorch 2.8 or higher</p></li>
<li><p>CUDA-capable GPU (for NCCL/NCCLX or RCCL backends)</p></li>
</ul>
</section>
<section id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Link to this heading">#</a></h2>
<p>torchcomms is available on PyPI and can be installed using pip. Alternatively,
you can build torchcomms from source.</p>
<section id="using-pip-nightly-builds">
<h3>Using pip (Nightly Builds)<a class="headerlink" href="#using-pip-nightly-builds" title="Link to this heading">#</a></h3>
<p>You can install torchcomms and PyTorch nightly builds using pip:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>--pre<span class="w"> </span>torch<span class="w"> </span>torchcomms<span class="w"> </span>--index-url<span class="w"> </span>https://download.pytorch.org/whl/nightly/cu128
</pre></div>
</div>
</section>
<section id="building-from-source">
<h3>Building from Source<a class="headerlink" href="#building-from-source" title="Link to this heading">#</a></h3>
<section id="id1">
<h4>Prerequisites<a class="headerlink" href="#id1" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>CMake 3.22 or higher</p></li>
<li><p>Ninja 1.10 or higher</p></li>
</ul>
<p>Alternatively, you can build torchcomms from source. If you want to build the NCCLX backend, we recommend building it under a virtual conda environment.
Run the following commands to build and install torchcomms:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a conda environment</span>
conda<span class="w"> </span>create<span class="w"> </span>-n<span class="w"> </span>torchcomms<span class="w"> </span><span class="nv">python</span><span class="o">=</span><span class="m">3</span>.10
conda<span class="w"> </span>activate<span class="w"> </span>torchcomms
<span class="c1"># Clone the repository</span>
git<span class="w"> </span>clone<span class="w"> </span>git@github.com:meta-pytorch/torchcomms.git
<span class="nb">cd</span><span class="w"> </span>torchcomms
</pre></div>
</div>
<p>Build the backend (choose one based on your hardware):</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-0" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-0">
Standard NCCL Backend</label><div class="sd-tab-content docutils">
<p>No build needed - uses the library provided by PyTorch</p>
</div>
<input id="sd-tab-item-1" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-1">
NCCLX Backend</label><div class="sd-tab-content docutils">
<p>If you want to install the third-party dependencies directly from conda, run the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">USE_SYSTEM_LIBS</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>./build_ncclx.sh
</pre></div>
</div>
<p>If you want to build and install the third-party dependencies from source, run the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./build_ncclx.sh
</pre></div>
</div>
</div>
<input id="sd-tab-item-2" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-2">
RCCL Backend</label><div class="sd-tab-content docutils">
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./build_rccl.sh
</pre></div>
</div>
</div>
</div>
<p>Install torchcomms:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install PyTorch (if not already installed)</span>
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
pip<span class="w"> </span>install<span class="w"> </span>-v<span class="w"> </span>.
</pre></div>
</div>
</section>
</section>
<section id="build-configuration">
<h3>Build Configuration<a class="headerlink" href="#build-configuration" title="Link to this heading">#</a></h3>
<p>You can customize the build by setting environment variables before running pip install:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Enable/disable specific backends (ON/OFF or 1/0)</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">USE_NCCL</span><span class="o">=</span>ON<span class="w">    </span><span class="c1"># Default: ON</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">USE_NCCLX</span><span class="o">=</span>ON<span class="w">   </span><span class="c1"># Default: ON</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">USE_GLOO</span><span class="o">=</span>ON<span class="w">    </span><span class="c1"># Default: ON</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">USE_RCCL</span><span class="o">=</span>OFF<span class="w">   </span><span class="c1"># Default: OFF</span>
</pre></div>
</div>
<p>Then run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install PyTorch (if not already installed)</span>
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
pip<span class="w"> </span>install<span class="w"> </span>-v<span class="w"> </span>.
</pre></div>
</div>
</section>
</section>
<section id="quick-start-example">
<h2>Quick Start Example<a class="headerlink" href="#quick-start-example" title="Link to this heading">#</a></h2>
<p>Here’s a simple example demonstrating synchronous <code class="docutils literal notranslate"><span class="pre">AllReduce</span></code> communication across multiple GPUs:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env python3</span>
<span class="c1"># example.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchcomms</span><span class="w"> </span><span class="kn">import</span> <span class="n">new_comm</span><span class="p">,</span> <span class="n">ReduceOp</span>

<span class="k">def</span><span class="w"> </span><span class="nf">main</span><span class="p">():</span>
    <span class="c1"># Initialize TorchComm with NCCLX backend</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
    <span class="n">torchcomm</span> <span class="o">=</span> <span class="n">new_comm</span><span class="p">(</span><span class="s2">&quot;ncclx&quot;</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;main_comm&quot;</span><span class="p">)</span>

    <span class="c1"># Get rank and world size</span>
    <span class="n">rank</span> <span class="o">=</span> <span class="n">torchcomm</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="n">torchcomm</span><span class="o">.</span><span class="n">get_size</span><span class="p">()</span>

    <span class="c1"># Calculate device ID</span>
    <span class="n">num_devices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>
    <span class="n">device_id</span> <span class="o">=</span> <span class="n">rank</span> <span class="o">%</span> <span class="n">num_devices</span>
    <span class="n">target_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;cuda:</span><span class="si">{</span><span class="n">device_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Rank </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">world_size</span><span class="si">}</span><span class="s2">: Running on device </span><span class="si">{</span><span class="n">device_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Create a tensor with rank-specific data</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">(</span>
        <span class="p">(</span><span class="mi">1024</span><span class="p">,),</span>
        <span class="nb">float</span><span class="p">(</span><span class="n">rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">target_device</span>
    <span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Rank </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">: Before AllReduce: </span><span class="si">{</span><span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Perform synchronous AllReduce (sum across all ranks)</span>
    <span class="n">torchcomm</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># Synchronize CUDA stream</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Rank </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">: After AllReduce: </span><span class="si">{</span><span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Cleanup</span>
    <span class="n">torchcomm</span><span class="o">.</span><span class="n">finalize</span><span class="p">()</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<section id="running-the-example">
<h3>Running the Example<a class="headerlink" href="#running-the-example" title="Link to this heading">#</a></h3>
<p>To run this example with multiple processes (one per GPU):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Using torchrun (recommended)</span>
torchrun<span class="w"> </span>--nproc_per_node<span class="o">=</span><span class="m">2</span><span class="w"> </span>example.py

<span class="c1"># Or using python -m torch.distributed.launch</span>
python<span class="w"> </span>-m<span class="w"> </span>torch.distributed.launch<span class="w"> </span>--nproc_per_node<span class="o">=</span><span class="m">2</span><span class="w"> </span>example.py
</pre></div>
</div>
<p>In the example above, we perform the following steps:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">new_comm()</span></code> creates a communicator with the specified backend</p></li>
<li><p>Each process gets its unique rank and total world size</p></li>
<li><p>Each rank creates a tensor with rank-specific values</p></li>
<li><p>All tensors are summed across all ranks</p></li>
<li><p>Clean up communication resources</p></li>
</ol>
</section>
</section>
<section id="asynchronous-operations">
<h2>Asynchronous Operations<a class="headerlink" href="#asynchronous-operations" title="Link to this heading">#</a></h2>
<p>torchcomms also supports asynchronous operations for better performance.
Here is the same example as above, but with asynchronous <code class="docutils literal notranslate"><span class="pre">AllReduce</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchcomms</span><span class="w"> </span><span class="kn">import</span> <span class="n">new_comm</span><span class="p">,</span> <span class="n">ReduceOp</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">torchcomm</span> <span class="o">=</span> <span class="n">new_comm</span><span class="p">(</span><span class="s2">&quot;ncclx&quot;</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;main_comm&quot;</span><span class="p">)</span>

<span class="n">rank</span> <span class="o">=</span> <span class="n">torchcomm</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span>
<span class="n">device_id</span> <span class="o">=</span> <span class="n">rank</span> <span class="o">%</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>
<span class="n">target_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;cuda:</span><span class="si">{</span><span class="n">device_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Create tensor</span>
<span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">1024</span><span class="p">,),</span> <span class="nb">float</span><span class="p">(</span><span class="n">rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">target_device</span><span class="p">)</span>

<span class="c1"># Start async AllReduce</span>
<span class="n">work</span> <span class="o">=</span> <span class="n">torchcomm</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Do other work while communication happens</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Rank </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">: Doing other work while AllReduce is in progress...&quot;</span><span class="p">)</span>

<span class="c1"># Wait for completion</span>
<span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Rank </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">: AllReduce completed&quot;</span><span class="p">)</span>

<span class="n">torchcomm</span><span class="o">.</span><span class="n">finalize</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="next-steps">
<h2>Next Steps<a class="headerlink" href="#next-steps" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Explore more examples in <a class="reference external" href="https://github.com/meta-pytorch/torchcomms/tree/main/comms/torchcomms/examples">torchcomms/comms/torchcomms/examples/</a></p></li>
<li><p>Check out the <a class="reference internal" href="api.html"><span class="doc std std-doc">torchcomms API documentation</span></a></p></li>
</ul>
</section>
</section>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
    <a class="left-prev"
       href="index.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">torchcomms</p>
      </div>
    </a>
    <a class="right-next"
       href="api.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">API Reference</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="index.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">torchcomms</p>
      </div>
    </a>
    <a class="right-next"
       href="api.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">API Reference</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prerequisites">Prerequisites</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#installation">Installation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-pip-nightly-builds">Using pip (Nightly Builds)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#building-from-source">Building from Source</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Prerequisites</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#build-configuration">Build Configuration</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quick-start-example">Quick Start Example</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#running-the-example">Running the Example</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#asynchronous-operations">Asynchronous Operations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#next-steps">Next Steps</a></li>
</ul>
  </nav></div>
    




</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  


<style>
.site-footer {
    padding: 20px 40px;
    height: 60px !important;
}

@media screen and (min-width: 768px) {
    .site-footer {
        padding: 20px 40px;
    }
}

.site-footer .privacy-policy {
    border-top: none;
    margin-top: 0px;
}

.site-footer .privacy-policy .copyright {
    padding-top: 0;
}
</style>


<footer class="site-footer">

    <div class="privacy-policy">
      <div class="copyright">
      
        <p>
           Copyright © 2025 Meta Platforms, Inc
        </p>
        
      </div>
    </div>


  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2025, torchcomms Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "Getting Started",
       "headline": "Getting Started",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/getting_started.html",
       "articleBody": "Getting Started# torchcomms is an experimental, lightweight communication API for PyTorchDistributed(PTD). It provides a simplified, object-oriented interface for distributed collective operations and offers both high-level collective APIs and multiple out-of-the-box backends. torchcomms provides: Simplified Object-Oriented API: A clean, intuitive interface for communication operations Support for Multiple Backends, including: NCCLX: NVIDIA Collective Communications Library (extended) - Meta\u2019s production-tested backend that powers all generative AI services NCCL: Standard NCCL backend for NVIDIA GPUs GLOO: CPU-based backend for CPU tensors and metadata transfer RCCL: AMD ROCm Collective Communications Library for AMD GPUs Synchronous and Asynchronous Operations: Flexible execution modes for different performance needs Native PyTorch Integration: Works seamlessly with PyTorch tensors and CUDA streams Scalable: Designed to scale to 100,000+ GPUs Common use cases for torchcomms include distributed training of neural networks, multi-GPU data parallelism, model parallelism across multiple devices, and collective communication patterns such as AllReduce, Broadcast, Send/Recv, and other operations. Prerequisites# torchcomms requires the following software and hardware: Python 3.10 or higher PyTorch 2.8 or higher CUDA-capable GPU (for NCCL/NCCLX or RCCL backends) Installation# torchcomms is available on PyPI and can be installed using pip. Alternatively, you can build torchcomms from source. Using pip (Nightly Builds)# You can install torchcomms and PyTorch nightly builds using pip: pip install --pre torch torchcomms --index-url https://download.pytorch.org/whl/nightly/cu128 Building from Source# Prerequisites# CMake 3.22 or higher Ninja 1.10 or higher Alternatively, you can build torchcomms from source. If you want to build the NCCLX backend, we recommend building it under a virtual conda environment. Run the following commands to build and install torchcomms: # Create a conda environment conda create -n torchcomms python=3.10 conda activate torchcomms # Clone the repository git clone git@github.com:meta-pytorch/torchcomms.git cd torchcomms Build the backend (choose one based on your hardware): Standard NCCL Backend No build needed - uses the library provided by PyTorch NCCLX Backend If you want to install the third-party dependencies directly from conda, run the following command: USE_SYSTEM_LIBS=1 ./build_ncclx.sh If you want to build and install the third-party dependencies from source, run the following command: ./build_ncclx.sh RCCL Backend ./build_rccl.sh Install torchcomms: # Install PyTorch (if not already installed) pip install -r requirements.txt pip install -v . Build Configuration# You can customize the build by setting environment variables before running pip install: # Enable/disable specific backends (ON/OFF or 1/0) export USE_NCCL=ON # Default: ON export USE_NCCLX=ON # Default: ON export USE_GLOO=ON # Default: ON export USE_RCCL=OFF # Default: OFF Then run: # Install PyTorch (if not already installed) pip install -r requirements.txt pip install -v . Quick Start Example# Here\u2019s a simple example demonstrating synchronous AllReduce communication across multiple GPUs: #!/usr/bin/env python3 # example.py import torch from torchcomms import new_comm, ReduceOp def main(): # Initialize TorchComm with NCCLX backend device = torch.device(\"cuda\") torchcomm = new_comm(\"ncclx\", device, name=\"main_comm\") # Get rank and world size rank = torchcomm.get_rank() world_size = torchcomm.get_size() # Calculate device ID num_devices = torch.cuda.device_count() device_id = rank % num_devices target_device = torch.device(f\"cuda:{device_id}\") print(f\"Rank {rank}/{world_size}: Running on device {device_id}\") # Create a tensor with rank-specific data tensor = torch.full( (1024,), float(rank + 1), dtype=torch.float32, device=target_device ) print(f\"Rank {rank}: Before AllReduce: {tensor[0].item()}\") # Perform synchronous AllReduce (sum across all ranks) torchcomm.all_reduce(tensor, ReduceOp.SUM, async_op=False) # Synchronize CUDA stream torch.cuda.current_stream().synchronize() print(f\"Rank {rank}: After AllReduce: {tensor[0].item()}\") # Cleanup torchcomm.finalize() if __name__ == \"__main__\": main() Running the Example# To run this example with multiple processes (one per GPU): # Using torchrun (recommended) torchrun --nproc_per_node=2 example.py # Or using python -m torch.distributed.launch python -m torch.distributed.launch --nproc_per_node=2 example.py In the example above, we perform the following steps: new_comm() creates a communicator with the specified backend Each process gets its unique rank and total world size Each rank creates a tensor with rank-specific values All tensors are summed across all ranks Clean up communication resources Asynchronous Operations# torchcomms also supports asynchronous operations for better performance. Here is the same example as above, but with asynchronous AllReduce: import torch from torchcomms import new_comm, ReduceOp device = torch.device(\"cuda\") torchcomm = new_comm(\"ncclx\", device, name=\"main_comm\") rank = torchcomm.get_rank() device_id = rank % torch.cuda.device_count() target_device = torch.device(f\"cuda:{device_id}\") # Create tensor tensor = torch.full((1024,), float(rank + 1), dtype=torch.float32, device=target_device) # Start async AllReduce work = torchcomm.all_reduce(tensor, ReduceOp.SUM, async_op=True) # Do other work while communication happens print(f\"Rank {rank}: Doing other work while AllReduce is in progress...\") # Wait for completion work.wait() print(f\"Rank {rank}: AllReduce completed\") torchcomm.finalize() Next Steps# Explore more examples in torchcomms/comms/torchcomms/examples/ Check out the torchcomms API documentation",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "https://pytorch.org/docs/stable/_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/getting_started.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>