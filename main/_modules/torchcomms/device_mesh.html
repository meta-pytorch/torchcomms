
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>torchcomms.device_mesh &#8212; meta-pytorch/torchcomms main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=047068a3" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=e14e8605" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=e14e8605" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=a8da1a53"></script>
    <script src="../../_static/doctools.js?v=888ff710"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";import elkLayouts from "https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs";mermaid.registerLayoutLoaders(elkLayouts);mermaid.initialize({startOnLoad:false});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";
window.addEventListener("load", () => mermaid.run());
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/torchcomms/device_mesh';</script>
    <link rel="canonical" href="https://meta-pytorch.org/torchcomms/main/_modules/torchcomms/device_mesh.html" />
    <link rel="icon" href="../../_static/torchcomms-logo-favicon.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<style>
  :root {
    --header-height: 0px !important;
    --header-height-desktop: 0px !important;
  }
</style>


<style>
  @media (min-width: 1100px) {
    .site-footer {
      height: 300px !important;
    }
  }
</style>

<link rel="stylesheet" type="text/css" href="../../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../../_static/js/theme.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="https://docs.pytorch.org/docs/stable/_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="torchcomms">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NPLPKN5G" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-NPLPKN5G');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'v0.1.0');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Repository configuration for tutorials -->

<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>

<body data-feedback-url="https://github.com/meta-pytorch/torchcomms" class="pytorch-body">
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo-light.png" class="logo__image only-light" alt="meta-pytorch/torchcomms main documentation - Home"/>
    <script>document.write(`<img src="../../_static/logo-dark.png" class="logo__image only-dark" alt="meta-pytorch/torchcomms main documentation - Home"/>`);</script>
  
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../getting_started.html">
    Getting Started
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../api.html">
    API Reference
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/meta-pytorch/torchcomms" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://dev-discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../getting_started.html">
    Getting Started
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../api.html">
    API Reference
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/meta-pytorch/torchcomms" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://dev-discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../index.html" class="nav-link">Module code</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">torchcomms.d...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="../index.html">
        <meta itemprop="name" content="Module code">
        <meta itemprop="position" content="1">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="torchcomms.device_mesh">
        <meta itemprop="position" content="2">
      </div>
    </div>

    
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for torchcomms.device_mesh</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Meta Platforms, Inc. and affiliates.</span>
<span class="c1"># pyre-strict</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">cast</span><span class="p">,</span> <span class="n">Optional</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.distributed</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dist</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed.device_mesh</span><span class="w"> </span><span class="kn">import</span> <span class="n">_mesh_resources</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed.distributed_c10d</span><span class="w"> </span><span class="kn">import</span> <span class="n">GroupName</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">torchcomms._comms</span><span class="w"> </span><span class="kn">import</span> <span class="n">_BackendWrapper</span><span class="p">,</span> <span class="n">_get_store</span><span class="p">,</span> <span class="n">new_comm</span><span class="p">,</span> <span class="n">TorchComm</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_create_torchcomm_process_group</span><span class="p">(</span>
    <span class="n">comm</span><span class="p">:</span> <span class="n">TorchComm</span><span class="p">,</span>
    <span class="n">group_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">backend_str</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;torchcomm&quot;</span><span class="p">,</span>
    <span class="n">prefix_store</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">object</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">global_ranks_mapping</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">dist</span><span class="o">.</span><span class="n">ProcessGroup</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper function to create a ProcessGroup backed by TorchComm and register it</span>
<span class="sd">    with the distributed runtime.</span>

<span class="sd">    Args:</span>
<span class="sd">        comm: TorchComm instance to wrap</span>
<span class="sd">        group_name: Name for the process group</span>
<span class="sd">        backend_str: Backend string identifier</span>
<span class="sd">        prefix_store: Store for the process group (can be None)</span>
<span class="sd">        global_ranks_mapping: Mapping from global rank to group rank</span>

<span class="sd">    Returns:</span>
<span class="sd">        The created and registered ProcessGroup instance</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Make the linter happy. GroupName is just an alias for str. The cost of</span>
    <span class="c1"># this conversion is negligible.</span>
    <span class="n">group_name</span> <span class="o">=</span> <span class="n">GroupName</span><span class="p">(</span><span class="n">group_name</span><span class="p">)</span>

    <span class="n">wrapper</span> <span class="o">=</span> <span class="n">_BackendWrapper</span><span class="p">(</span><span class="n">comm</span><span class="p">)</span>  <span class="c1"># noqa: F405</span>
    <span class="n">backend_type</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">ProcessGroup</span><span class="o">.</span><span class="n">BackendType</span><span class="o">.</span><span class="n">CUSTOM</span>  <span class="c1"># noqa: F841</span>
    <span class="n">backend_config</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">BackendConfig</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">Backend</span><span class="p">(</span><span class="n">backend_str</span><span class="p">))</span>

    <span class="c1"># Create process group</span>
    <span class="c1"># pyre-fixme[6]: support store=None</span>
    <span class="n">pg</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">ProcessGroup</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">comm</span><span class="o">.</span><span class="n">get_rank</span><span class="p">(),</span> <span class="n">comm</span><span class="o">.</span><span class="n">get_size</span><span class="p">())</span>

    <span class="c1"># Register backend</span>
    <span class="c1"># pyre-fixme[6]: BackendWrapper implements dist.Backend but types isn&#39;t aware</span>
    <span class="n">pg</span><span class="o">.</span><span class="n">_register_backend</span><span class="p">(</span><span class="n">comm</span><span class="o">.</span><span class="n">get_device</span><span class="p">(),</span> <span class="n">backend_type</span><span class="p">,</span> <span class="n">wrapper</span><span class="p">)</span>
    <span class="n">pg</span><span class="o">.</span><span class="n">_set_group_name</span><span class="p">(</span><span class="n">group_name</span><span class="p">)</span>

    <span class="c1"># Update global state</span>
    <span class="c1"># pyre-fixme[6]: support store=None</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">distributed_c10d</span><span class="o">.</span><span class="n">_world</span><span class="o">.</span><span class="n">pg_map</span><span class="p">[</span><span class="n">pg</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">backend_str</span><span class="p">,</span> <span class="n">prefix_store</span><span class="p">)</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">distributed_c10d</span><span class="o">.</span><span class="n">_world</span><span class="o">.</span><span class="n">pg_names</span><span class="p">[</span><span class="n">pg</span><span class="p">]</span> <span class="o">=</span> <span class="n">group_name</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">distributed_c10d</span><span class="o">.</span><span class="n">_world</span><span class="o">.</span><span class="n">pg_backend_config</span><span class="p">[</span><span class="n">pg</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">backend_config</span><span class="p">)</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">distributed_c10d</span><span class="o">.</span><span class="n">_register_process_group</span><span class="p">(</span><span class="n">group_name</span><span class="p">,</span> <span class="n">pg</span><span class="p">)</span>

    <span class="c1"># Set up rank mapping</span>
    <span class="k">if</span> <span class="n">global_ranks_mapping</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">distributed_c10d</span><span class="o">.</span><span class="n">_world</span><span class="o">.</span><span class="n">pg_group_ranks</span><span class="p">[</span><span class="n">pg</span><span class="p">]</span> <span class="o">=</span> <span class="n">global_ranks_mapping</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Default mapping for global process groups</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">distributed_c10d</span><span class="o">.</span><span class="n">_world</span><span class="o">.</span><span class="n">pg_group_ranks</span><span class="p">[</span><span class="n">pg</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">i</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">comm</span><span class="o">.</span><span class="n">get_size</span><span class="p">())</span>
        <span class="p">}</span>

    <span class="c1"># Set up process group tag</span>
    <span class="n">pg_tag</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;ptd:</span><span class="si">{</span><span class="n">group_name</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">distributed_c10d</span><span class="o">.</span><span class="n">_world</span><span class="o">.</span><span class="n">tags_to_pg</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="n">pg_tag</span><span class="p">,</span> <span class="p">[])</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pg</span><span class="p">)</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">distributed_c10d</span><span class="o">.</span><span class="n">_world</span><span class="o">.</span><span class="n">pg_to_tag</span><span class="p">[</span><span class="n">pg</span><span class="p">]</span> <span class="o">=</span> <span class="n">pg_tag</span>

    <span class="k">return</span> <span class="n">pg</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_store_for_pg</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">dist</span><span class="o">.</span><span class="n">Store</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">_get_store_for_pg</span><span class="p">,</span> <span class="s2">&quot;_store&quot;</span><span class="p">):</span>
        <span class="n">_get_store_for_pg</span><span class="o">.</span><span class="n">_store</span> <span class="o">=</span> <span class="n">_get_store</span><span class="p">(</span>  <span class="c1"># pyre-ignore[16]</span>
            <span class="s2">&quot;torchcomm&quot;</span><span class="p">,</span> <span class="s2">&quot;store_dist&quot;</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="n">_get_store_for_pg</span><span class="o">.</span><span class="n">_store</span>


<div class="viewcode-block" id="init_device_mesh">
<a class="viewcode-back" href="../../api.html#torchcomms.device_mesh.init_device_mesh">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">init_device_mesh</span><span class="p">(</span>
    <span class="n">mesh_dim_comms</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">TorchComm</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>  <span class="c1"># noqa: F405</span>
    <span class="n">mesh_dim_names</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">_global_comm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchComm</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># noqa: F405</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">dist</span><span class="o">.</span><span class="n">DeviceMesh</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes a `DeviceMesh` from the list of provided `TorchComm` instances.</span>

<span class="sd">    See `DeviceMesh` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">device</span> <span class="o">=</span> <span class="n">mesh_dim_comms</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_device</span><span class="p">()</span>
    <span class="n">mesh_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">comm</span><span class="o">.</span><span class="n">get_size</span><span class="p">()</span> <span class="k">for</span> <span class="n">comm</span> <span class="ow">in</span> <span class="n">mesh_dim_comms</span><span class="p">)</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">mesh_shape</span><span class="p">)</span>

    <span class="n">mesh</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">world_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">mesh_shape</span><span class="p">)</span>

    <span class="n">local_ranks</span> <span class="o">=</span> <span class="p">[</span><span class="n">comm</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span> <span class="k">for</span> <span class="n">comm</span> <span class="ow">in</span> <span class="n">mesh_dim_comms</span><span class="p">]</span>
    <span class="n">global_rank</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">mesh</span><span class="p">[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">local_ranks</span><span class="p">)]</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">prefix_store</span> <span class="o">=</span> <span class="n">_get_store_for_pg</span><span class="p">()</span>
    <span class="n">backend_str</span> <span class="o">=</span> <span class="s2">&quot;torchcomm&quot;</span>
    <span class="c1"># Register the backend</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">Backend</span><span class="o">.</span><span class="n">register_backend</span><span class="p">(</span><span class="n">backend_str</span><span class="p">,</span> <span class="n">new_comm</span><span class="p">)</span>

    <span class="n">global_pg</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">_global_comm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">global_pg</span> <span class="o">=</span> <span class="n">_create_torchcomm_process_group</span><span class="p">(</span>
            <span class="n">comm</span><span class="o">=</span><span class="n">_global_comm</span><span class="p">,</span>
            <span class="n">group_name</span><span class="o">=</span><span class="n">_global_comm</span><span class="o">.</span><span class="n">get_name</span><span class="p">(),</span>
            <span class="n">prefix_store</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">PrefixStore</span><span class="p">(</span><span class="s2">&quot;default&quot;</span><span class="p">,</span> <span class="n">prefix_store</span><span class="p">),</span>
            <span class="n">global_ranks_mapping</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># Will use default mapping</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">mesh_dim_comms</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;More than one torch comm objects are passed but no global comm(_global_comm) is provided. &quot;</span>
            <span class="s2">&quot;Please provide a global comm object via _global_comm.&quot;</span>
        <span class="p">)</span>

    <span class="n">group_names</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">comm</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">mesh_dim_comms</span><span class="p">,</span> <span class="n">mesh_dim_names</span><span class="p">):</span>
        <span class="n">group_name</span> <span class="o">=</span> <span class="n">name</span>

        <span class="c1"># Calculate global ranks mapping for this mesh dimension</span>
        <span class="n">global_ranks</span> <span class="o">=</span> <span class="n">mesh</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">mesh</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">idx</span><span class="p">))</span>
        <span class="c1"># Find the row containing the global rank</span>
        <span class="n">row_idx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">global_ranks</span> <span class="o">==</span> <span class="n">global_rank</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="n">list_rank</span> <span class="o">=</span> <span class="n">global_ranks</span><span class="p">[</span><span class="n">row_idx</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="n">global_ranks_mapping</span> <span class="o">=</span> <span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">j</span> <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">list_rank</span><span class="p">)}</span>

        <span class="c1"># Use helper function to create the process group</span>
        <span class="n">pg</span> <span class="o">=</span> <span class="n">_create_torchcomm_process_group</span><span class="p">(</span>
            <span class="n">comm</span><span class="o">=</span><span class="n">comm</span><span class="p">,</span>
            <span class="n">group_name</span><span class="o">=</span><span class="n">group_name</span><span class="p">,</span>
            <span class="n">backend_str</span><span class="o">=</span><span class="n">backend_str</span><span class="p">,</span>
            <span class="n">prefix_store</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">PrefixStore</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">prefix_store</span><span class="p">),</span>
            <span class="n">global_ranks_mapping</span><span class="o">=</span><span class="n">global_ranks_mapping</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">_global_comm</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">global_pg</span> <span class="o">=</span> <span class="n">pg</span>

        <span class="n">group_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">group_name</span><span class="p">)</span>
        <span class="n">idx</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="c1"># Set as the default world process group</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">distributed_c10d</span><span class="o">.</span><span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span> <span class="o">=</span> <span class="n">global_pg</span>

    <span class="n">device_mesh</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">DeviceMesh</span><span class="p">(</span>
        <span class="n">device_type</span><span class="o">=</span><span class="n">device</span><span class="o">.</span><span class="n">type</span><span class="p">,</span>
        <span class="n">mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">,</span>
        <span class="n">mesh_dim_names</span><span class="o">=</span><span class="n">mesh_dim_names</span><span class="p">,</span>
        <span class="n">_init_backend</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">_rank</span><span class="o">=</span><span class="n">global_rank</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">device_mesh</span><span class="o">.</span><span class="n">_dim_group_names</span> <span class="o">=</span> <span class="n">group_names</span>

    <span class="k">return</span> <span class="n">device_mesh</span></div>



<span class="k">def</span><span class="w"> </span><span class="nf">_flatten_with_comm</span><span class="p">(</span>
    <span class="n">mesh</span><span class="p">:</span> <span class="n">dist</span><span class="o">.</span><span class="n">DeviceMesh</span><span class="p">,</span>
    <span class="n">mesh_dim_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">comm</span><span class="p">:</span> <span class="n">TorchComm</span><span class="p">,</span>  <span class="c1"># noqa: F405</span>
    <span class="n">global_ranks</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">layout</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>  <span class="c1"># noqa: F405</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">dist</span><span class="o">.</span><span class="n">DeviceMesh</span><span class="p">:</span>
    <span class="n">backend_str</span> <span class="o">=</span> <span class="s2">&quot;torchcomm&quot;</span>
    <span class="n">prefix_store</span> <span class="o">=</span> <span class="n">_get_store_for_pg</span><span class="p">()</span>
    <span class="n">prefix_store</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">PrefixStore</span><span class="p">(</span><span class="n">mesh_dim_name</span><span class="p">,</span> <span class="n">prefix_store</span><span class="p">)</span>
    <span class="n">global_ranks_mapping</span> <span class="o">=</span> <span class="p">{</span><span class="n">global_ranks</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">comm</span><span class="o">.</span><span class="n">get_size</span><span class="p">())}</span>
    <span class="c1"># We still need to register the process group for the flattened mesh</span>
    <span class="n">_create_torchcomm_process_group</span><span class="p">(</span>
        <span class="n">comm</span><span class="o">=</span><span class="n">comm</span><span class="p">,</span>
        <span class="n">group_name</span><span class="o">=</span><span class="n">mesh_dim_name</span><span class="p">,</span>
        <span class="n">backend_str</span><span class="o">=</span><span class="n">backend_str</span><span class="p">,</span>
        <span class="n">prefix_store</span><span class="o">=</span><span class="n">prefix_store</span><span class="p">,</span>
        <span class="n">global_ranks_mapping</span><span class="o">=</span><span class="n">global_ranks_mapping</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># We had a refactor recently that changed the way we create a DeviceMesh</span>
    <span class="c1"># We need to create a new DeviceMesh with the new API.</span>
    <span class="c1"># TODO: Clean up this code once torchcomm releases.</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="s2">&quot;_rank_map&quot;</span><span class="p">):</span>
        <span class="n">flattened_device_mesh</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">DeviceMesh</span><span class="p">(</span>
            <span class="n">device_type</span><span class="o">=</span><span class="n">comm</span><span class="o">.</span><span class="n">get_device</span><span class="p">(),</span>
            <span class="n">mesh_dim_names</span><span class="o">=</span><span class="p">(</span><span class="n">mesh_dim_name</span><span class="p">,),</span>
            <span class="n">_init_backend</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">_rank</span><span class="o">=</span><span class="n">comm</span><span class="o">.</span><span class="n">get_rank</span><span class="p">(),</span>
            <span class="n">_layout</span><span class="o">=</span><span class="n">layout</span><span class="o">.</span><span class="n">coalesce</span><span class="p">(),</span>
            <span class="n">_rank_map</span><span class="o">=</span><span class="n">mesh</span><span class="o">.</span><span class="n">_rank_map</span><span class="p">,</span>
            <span class="n">_root_mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">flattened_device_mesh</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">DeviceMesh</span><span class="p">(</span>
            <span class="n">device_type</span><span class="o">=</span><span class="n">comm</span><span class="o">.</span><span class="n">get_device</span><span class="p">(),</span>
            <span class="n">mesh</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">global_ranks</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">),</span>
            <span class="n">mesh_dim_names</span><span class="o">=</span><span class="p">(</span><span class="n">mesh_dim_name</span><span class="p">,),</span>
            <span class="n">_init_backend</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">_rank</span><span class="o">=</span><span class="n">comm</span><span class="o">.</span><span class="n">get_rank</span><span class="p">(),</span>
            <span class="n">_layout</span><span class="o">=</span><span class="n">layout</span><span class="o">.</span><span class="n">coalesce</span><span class="p">(),</span>
        <span class="p">)</span>
    <span class="n">flattened_device_mesh</span><span class="o">.</span><span class="n">_dim_group_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">mesh_dim_name</span><span class="p">]</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">flattened_device_mesh</span><span class="o">.</span><span class="n">_root_mesh</span> <span class="o">=</span> <span class="n">mesh</span><span class="o">.</span><span class="n">_get_root_mesh</span><span class="p">()</span>
        <span class="n">flattened_device_mesh</span><span class="o">.</span><span class="n">_root_mesh</span><span class="o">.</span><span class="n">_flatten_mapping</span><span class="p">[</span><span class="n">mesh_dim_name</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">flattened_device_mesh</span>
        <span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">_mesh_resources</span><span class="p">,</span> <span class="s2">&quot;flatten_name_to_root_dims&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="s2">&quot;Flattening with torchcomm is not supported for device mesh without mesh layout.&quot;</span>
            <span class="p">)</span>
        <span class="n">root_mesh</span> <span class="o">=</span> <span class="n">_mesh_resources</span><span class="o">.</span><span class="n">get_root_mesh</span><span class="p">(</span><span class="n">mesh</span><span class="p">)</span>
        <span class="n">_mesh_resources</span><span class="o">.</span><span class="n">child_to_root_mapping</span><span class="p">[</span>  <span class="c1"># pyre-ignore[16]</span>
            <span class="n">flattened_device_mesh</span>
        <span class="p">]</span> <span class="o">=</span> <span class="n">root_mesh</span>
        <span class="n">_mesh_resources</span><span class="o">.</span><span class="n">root_to_flatten_mapping</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span>  <span class="c1"># pyre-ignore[16]</span>
            <span class="n">root_mesh</span><span class="p">,</span> <span class="p">{}</span>
        <span class="p">)[</span><span class="n">mesh_dim_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">flattened_device_mesh</span>

    <span class="k">return</span> <span class="n">flattened_device_mesh</span>
</pre></div>

                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
</div>

<div class="footer-info">
  <p class="copyright">
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  


<style>
.site-footer {
    padding: 20px 40px;
    height: 60px !important;
}

@media screen and (min-width: 768px) {
    .site-footer {
        padding: 20px 40px;
    }
}

.site-footer .privacy-policy {
    border-top: none;
    margin-top: 0px;
}

.site-footer .privacy-policy .copyright {
    padding-top: 0;
}
</style>


<footer class="site-footer">

    <div class="privacy-policy">
      <div class="copyright">
      
        <p>
           Copyright © 2025 Meta Platforms, Inc
        </p>
        
      </div>
    </div>


  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2025, torchcomms Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "torchcomms.device_mesh",
       "headline": "torchcomms.device_mesh",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/_modules/torchcomms/device_mesh.html",
       "articleBody": "Source code for torchcomms.device_mesh # Copyright (c) Meta Platforms, Inc. and affiliates. # pyre-strict import math from typing import Any, cast, Optional import torch import torch.distributed as dist from torch.distributed.device_mesh import _mesh_resources from torch.distributed.distributed_c10d import GroupName from torchcomms._comms import _BackendWrapper, _get_store, new_comm, TorchComm def _create_torchcomm_process_group( comm: TorchComm, group_name: str, backend_str: str = \"torchcomm\", prefix_store: Optional[object] = None, global_ranks_mapping: Optional[dict[int, int]] = None, ) -\u003e dist.ProcessGroup: \"\"\" Helper function to create a ProcessGroup backed by TorchComm and register it with the distributed runtime. Args: comm: TorchComm instance to wrap group_name: Name for the process group backend_str: Backend string identifier prefix_store: Store for the process group (can be None) global_ranks_mapping: Mapping from global rank to group rank Returns: The created and registered ProcessGroup instance \"\"\" # Make the linter happy. GroupName is just an alias for str. The cost of # this conversion is negligible. group_name = GroupName(group_name) wrapper = _BackendWrapper(comm) # noqa: F405 backend_type = dist.ProcessGroup.BackendType.CUSTOM # noqa: F841 backend_config = dist.BackendConfig(dist.Backend(backend_str)) # Create process group # pyre-fixme[6]: support store=None pg = dist.ProcessGroup(None, comm.get_rank(), comm.get_size()) # Register backend # pyre-fixme[6]: BackendWrapper implements dist.Backend but types isn\u0027t aware pg._register_backend(comm.get_device(), backend_type, wrapper) pg._set_group_name(group_name) # Update global state # pyre-fixme[6]: support store=None dist.distributed_c10d._world.pg_map[pg] = (backend_str, prefix_store) dist.distributed_c10d._world.pg_names[pg] = group_name dist.distributed_c10d._world.pg_backend_config[pg] = str(backend_config) dist.distributed_c10d._register_process_group(group_name, pg) # Set up rank mapping if global_ranks_mapping is not None: dist.distributed_c10d._world.pg_group_ranks[pg] = global_ranks_mapping else: # Default mapping for global process groups dist.distributed_c10d._world.pg_group_ranks[pg] = { i: i for i in range(comm.get_size()) } # Set up process group tag pg_tag = f\"ptd:{group_name}\" dist.distributed_c10d._world.tags_to_pg.setdefault(pg_tag, []).append(pg) dist.distributed_c10d._world.pg_to_tag[pg] = pg_tag return pg def _get_store_for_pg() -\u003e dist.Store: if not hasattr(_get_store_for_pg, \"_store\"): _get_store_for_pg._store = _get_store( # pyre-ignore[16] \"torchcomm\", \"store_dist\" ) return _get_store_for_pg._store [docs] def init_device_mesh( mesh_dim_comms: tuple[TorchComm, ...], # noqa: F405 mesh_dim_names: tuple[str, ...], _global_comm: Optional[TorchComm] = None, # noqa: F405 ) -\u003e dist.DeviceMesh: \"\"\" Initializes a `DeviceMesh` from the list of provided `TorchComm` instances. See `DeviceMesh` for more details. \"\"\" device = mesh_dim_comms[0].get_device() mesh_shape = tuple(comm.get_size() for comm in mesh_dim_comms) world_size = math.prod(mesh_shape) mesh = torch.arange(world_size, dtype=torch.int, device=\"cpu\").view(mesh_shape) local_ranks = [comm.get_rank() for comm in mesh_dim_comms] global_rank = cast(int, mesh[tuple(local_ranks)].item()) prefix_store = _get_store_for_pg() backend_str = \"torchcomm\" # Register the backend dist.Backend.register_backend(backend_str, new_comm) global_pg = None if _global_comm is not None: global_pg = _create_torchcomm_process_group( comm=_global_comm, group_name=_global_comm.get_name(), prefix_store=dist.PrefixStore(\"default\", prefix_store), global_ranks_mapping=None, # Will use default mapping ) elif len(mesh_dim_comms) != 1: raise RuntimeError( \"More than one torch comm objects are passed but no global comm(_global_comm) is provided. \" \"Please provide a global comm object via _global_comm.\" ) group_names = [] idx = 0 for comm, name in zip(mesh_dim_comms, mesh_dim_names): group_name = name # Calculate global ranks mapping for this mesh dimension global_ranks = mesh.transpose(idx, -1).reshape(-1, mesh.size(idx)) # Find the row containing the global rank row_idx = int(torch.where(global_ranks == global_rank)[0].item()) list_rank = global_ranks[row_idx].tolist() global_ranks_mapping = {x: j for j, x in enumerate(list_rank)} # Use helper function to create the process group pg = _create_torchcomm_process_group( comm=comm, group_name=group_name, backend_str=backend_str, prefix_store=dist.PrefixStore(name, prefix_store), global_ranks_mapping=global_ranks_mapping, ) if _global_comm is None and idx == 0: global_pg = pg group_names.append(group_name) idx += 1 # Set as the default world process group dist.distributed_c10d.GroupMember.WORLD = global_pg device_mesh = dist.DeviceMesh( device_type=device.type, mesh=mesh, mesh_dim_names=mesh_dim_names, _init_backend=False, _rank=global_rank, ) device_mesh._dim_group_names = group_names return device_mesh def _flatten_with_comm( mesh: dist.DeviceMesh, mesh_dim_name: str, comm: TorchComm, # noqa: F405 global_ranks: list[int], layout: Any, # noqa: F405 ) -\u003e dist.DeviceMesh: backend_str = \"torchcomm\" prefix_store = _get_store_for_pg() prefix_store = dist.PrefixStore(mesh_dim_name, prefix_store) global_ranks_mapping = {global_ranks[i]: i for i in range(comm.get_size())} # We still need to register the process group for the flattened mesh _create_torchcomm_process_group( comm=comm, group_name=mesh_dim_name, backend_str=backend_str, prefix_store=prefix_store, global_ranks_mapping=global_ranks_mapping, ) # We had a refactor recently that changed the way we create a DeviceMesh # We need to create a new DeviceMesh with the new API. # TODO: Clean up this code once torchcomm releases. if hasattr(mesh, \"_rank_map\"): flattened_device_mesh = dist.DeviceMesh( device_type=comm.get_device(), mesh_dim_names=(mesh_dim_name,), _init_backend=False, _rank=comm.get_rank(), _layout=layout.coalesce(), _rank_map=mesh._rank_map, _root_mesh=mesh, ) else: flattened_device_mesh = dist.DeviceMesh( device_type=comm.get_device(), mesh=torch.tensor(global_ranks, device=\"cpu\"), mesh_dim_names=(mesh_dim_name,), _init_backend=False, _rank=comm.get_rank(), _layout=layout.coalesce(), ) flattened_device_mesh._dim_group_names = [mesh_dim_name] try: flattened_device_mesh._root_mesh = mesh._get_root_mesh() flattened_device_mesh._root_mesh._flatten_mapping[mesh_dim_name] = ( flattened_device_mesh ) except Exception: if hasattr(_mesh_resources, \"flatten_name_to_root_dims\"): raise NotImplementedError( \"Flattening with torchcomm is not supported for device mesh without mesh layout.\" ) root_mesh = _mesh_resources.get_root_mesh(mesh) _mesh_resources.child_to_root_mapping[ # pyre-ignore[16] flattened_device_mesh ] = root_mesh _mesh_resources.root_to_flatten_mapping.setdefault( # pyre-ignore[16] root_mesh, {} )[mesh_dim_name] = flattened_device_mesh return flattened_device_mesh",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "https://pytorch.org/docs/stable/_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/_modules/torchcomms/device_mesh.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>