
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>API Reference &#8212; meta-pytorch/torchcomms main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=047068a3" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css?v=e14e8605" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css?v=e14e8605" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=a8da1a53"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'api';</script>
    <link rel="canonical" href="https://meta-pytorch.org/torchcomms/main/api.html" />
    <link rel="icon" href="_static/torchcomms-logo-favicon.png"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Getting Started" href="getting_started.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<style>
  :root {
    --header-height: 0px !important;
    --header-height-desktop: 0px !important;
  }
</style>


<style>
  @media (min-width: 1100px) {
    .site-footer {
      height: 300px !important;
    }
  }
</style>

<link rel="stylesheet" type="text/css" href="_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="_static/js/theme.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="https://docs.pytorch.org/docs/stable/_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="torchcomms">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NPLPKN5G" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-NPLPKN5G');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'v0.1.0');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Repository configuration for tutorials -->

<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>

<body data-feedback-url="https://github.com/meta-pytorch/torchcomms" class="pytorch-body">
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo-light.png" class="logo__image only-light" alt="meta-pytorch/torchcomms main documentation - Home"/>
    <script>document.write(`<img src="_static/logo-dark.png" class="logo__image only-dark" alt="meta-pytorch/torchcomms main documentation - Home"/>`);</script>
  
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="getting_started.html">
    Getting Started
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="#">
    API Reference
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/meta-pytorch/torchcomms" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://dev-discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="getting_started.html">
    Getting Started
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="#">
    API Reference
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/meta-pytorch/torchcomms" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://dev-discuss.pytorch.org/" title="Discourse" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discourse</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"></div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page">API Reference</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="API Reference">
        <meta itemprop="position" content="1">
      </div>
    </div>

    
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="module-torchcomms">
<span id="api-reference"></span><h1>API Reference<a class="headerlink" href="#module-torchcomms" title="Link to this heading">#</a></h1>
<dl class="py function">
<dt class="sig sig-object py" id="torchcomms.new_comm">
<span class="sig-prename descclassname"><span class="pre">torchcomms.</span></span><span class="sig-name descname"><span class="pre">new_comm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">backend</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">abort_process_on_timeout_or_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">datetime.timedelta</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">high_priority_stream</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">store</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.distributed.distributed_c10d.Store</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hints</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">collections.abc.Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">torchcomms.TorchComm</span></a></span></span><a class="headerlink" href="#torchcomms.new_comm" title="Link to this definition">#</a></dt>
<dd><p>Create a new communicator.</p>
<p>This requires all ranks that will be part of the commmunicator call this
function simultaneously.</p>
<p>Ranks and world size will be derived from environment variables set by launchers
such as torchrun (i.e. <code class="docutils literal notranslate"><span class="pre">RANK</span></code>, <code class="docutils literal notranslate"><span class="pre">WORLD_SIZE</span></code>).</p>
<p>Backends typically use a store to initialize which can either be provided or
automatically instantiated from environment variables such as <code class="docutils literal notranslate"><span class="pre">MASTER_ADDR</span></code>
and <code class="docutils literal notranslate"><span class="pre">MASTER_PORT</span></code>. Backends are not required to use the store to initialize if
more performant options are available.</p>
<p>Subcommunicators can be instantiated by using the <code class="docutils literal notranslate"><span class="pre">split</span></code> method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>backend</strong> (<em>str</em>) – The backend to use for the communicator.</p></li>
<li><p><strong>device</strong> (<em>torch.device</em>) – The device to use for the communicator.</p></li>
<li><p><strong>name</strong> (<em>str</em>) – The name of the communicator. This must be unique within the process.</p></li>
<li><p><strong>abort_process_on_timeout_or_error</strong> (<em>bool</em>) – Whether to abort process on timeout or error.</p></li>
<li><p><strong>timeout</strong> (<em>timedelta</em>) – Timeout for initialization.</p></li>
<li><p><strong>high_priority_stream</strong> (<em>bool</em>) – Whether to use high priority stream.</p></li>
<li><p><strong>store</strong> (<em>torch.distributed.Store</em>) – Store used to initialize the communicator between processes.</p></li>
<li><p><strong>hints</strong> (<em>dict</em>) – Dictionary of string hints for backend-specific options.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchcomms.TorchComm">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchcomms.</span></span><span class="sig-name descname"><span class="pre">TorchComm</span></span><a class="headerlink" href="#torchcomms.TorchComm" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">pybind11_object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchComm.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchcomms.TorchComm.__init__" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchComm.all_gather">
<span class="sig-name descname"><span class="pre">all_gather</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">torchcomms.TorchComm</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor_list</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">collections.abc.Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">async_op</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hints</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">collections.abc.Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">datetime.timedelta</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchcomms.TorchWork" title="torchcomms.TorchWork"><span class="pre">torchcomms.TorchWork</span></a></span></span><a class="headerlink" href="#torchcomms.TorchComm.all_gather" title="Link to this definition">#</a></dt>
<dd><p>Gather a tensor from all ranks in the communicator.</p>
<p>Output will be available on all ranks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor_list</strong> – the list of tensors to gather into</p></li>
<li><p><strong>tensor</strong> – the input tensor to share</p></li>
<li><p><strong>async_op</strong> – whether to perform the operation asynchronously</p></li>
<li><p><strong>hints</strong> – dictionary of string hints for backend-specific options</p></li>
<li><p><strong>timeout</strong> – timeout for the operation</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchComm.all_gather_single">
<span class="sig-name descname"><span class="pre">all_gather_single</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">torchcomms.TorchComm</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">output</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">async_op</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hints</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">collections.abc.Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">datetime.timedelta</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchcomms.TorchWork" title="torchcomms.TorchWork"><span class="pre">torchcomms.TorchWork</span></a></span></span><a class="headerlink" href="#torchcomms.TorchComm.all_gather_single" title="Link to this definition">#</a></dt>
<dd><p>Gather a single tensor from all ranks in the communicator.</p>
<p>The output tensor must be of size (world_size * input.numel()).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output</strong> – the output tensor to gather into</p></li>
<li><p><strong>input</strong> – the input tensor to share</p></li>
<li><p><strong>async_op</strong> – whether to perform the operation asynchronously</p></li>
<li><p><strong>hints</strong> – dictionary of string hints for backend-specific options</p></li>
<li><p><strong>timeout</strong> – timeout for the operation</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchComm.all_gather_v">
<span class="sig-name descname"><span class="pre">all_gather_v</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">torchcomms.TorchComm</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor_list</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">collections.abc.Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">async_op</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hints</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">collections.abc.Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">datetime.timedelta</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchcomms.TorchWork" title="torchcomms.TorchWork"><span class="pre">torchcomms.TorchWork</span></a></span></span><a class="headerlink" href="#torchcomms.TorchComm.all_gather_v" title="Link to this definition">#</a></dt>
<dd><p>Gather a tensor from all ranks in the communicator, supporting variable tensor sizes per rank.</p>
<p>Output will be available on all ranks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor_list</strong> – the list of tensors to gather into; the list is the same on all ranks, but tensor sizes may differ between indices.</p></li>
<li><p><strong>tensor</strong> – the input tensor to share; size may differ per rank.</p></li>
<li><p><strong>async_op</strong> – whether to perform the operation asynchronously</p></li>
<li><p><strong>hints</strong> – dictionary of string hints for backend-specific options</p></li>
<li><p><strong>timeout</strong> – timeout for the operation</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchComm.all_reduce">
<span class="sig-name descname"><span class="pre">all_reduce</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">torchcomms.TorchComm</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">op</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.ReduceOp" title="torchcomms.ReduceOp"><span class="pre">torchcomms.ReduceOp</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">async_op</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hints</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">collections.abc.Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">datetime.timedelta</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchcomms.TorchWork" title="torchcomms.TorchWork"><span class="pre">torchcomms.TorchWork</span></a></span></span><a class="headerlink" href="#torchcomms.TorchComm.all_reduce" title="Link to this definition">#</a></dt>
<dd><p>Reduce a tensor across all ranks in the communicator.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> – the tensor to all-reduce</p></li>
<li><p><strong>op</strong> – the reduction operation</p></li>
<li><p><strong>async_op</strong> – whether to perform the operation asynchronously</p></li>
<li><p><strong>hints</strong> – dictionary of string hints for backend-specific options</p></li>
<li><p><strong>timeout</strong> – timeout for the operation</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchComm.all_to_all">
<span class="sig-name descname"><span class="pre">all_to_all</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">torchcomms.TorchComm</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_tensor_list</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">collections.abc.Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_tensor_list</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">collections.abc.Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">async_op</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hints</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">collections.abc.Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">datetime.timedelta</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchcomms.TorchWork" title="torchcomms.TorchWork"><span class="pre">torchcomms.TorchWork</span></a></span></span><a class="headerlink" href="#torchcomms.TorchComm.all_to_all" title="Link to this definition">#</a></dt>
<dd><p>Scatter the split list to all ranks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_tensor_list</strong> – Output tensor list.</p></li>
<li><p><strong>input_tensor_list</strong> – Input tensor list to scatter.</p></li>
<li><p><strong>async_op</strong> – Whether to perform the operation asynchronously.</p></li>
<li><p><strong>hints</strong> – Dictionary of string hints for backend-specific options.</p></li>
<li><p><strong>timeout</strong> – Timeout for the operation.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchComm.all_to_all_single">
<span class="sig-name descname"><span class="pre">all_to_all_single</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">torchcomms.TorchComm</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">output</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">async_op</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hints</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">collections.abc.Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">datetime.timedelta</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchcomms.TorchWork" title="torchcomms.TorchWork"><span class="pre">torchcomms.TorchWork</span></a></span></span><a class="headerlink" href="#torchcomms.TorchComm.all_to_all_single" title="Link to this definition">#</a></dt>
<dd><p>Split input tensor and then scatter the split list to all ranks.</p>
<p>Later the received tensors are concatenated and returned as a single
output tensor.</p>
<p>The input and output tensor sizes must a multiple of world_size.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output</strong> – Output tensor.</p></li>
<li><p><strong>input</strong> – Input tensor to split and scatter.</p></li>
<li><p><strong>async_op</strong> – Whether to perform the operation asynchronously.</p></li>
<li><p><strong>hints</strong> – Dictionary of string hints for backend-specific options.</p></li>
<li><p><strong>timeout</strong> – Timeout for the operation.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchComm.all_to_all_v_single">
<span class="sig-name descname"><span class="pre">all_to_all_v_single</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">torchcomms.TorchComm</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">output</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_split_sizes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">collections.abc.Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">SupportsInt</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_split_sizes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">collections.abc.Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">SupportsInt</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">async_op</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hints</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">collections.abc.Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">datetime.timedelta</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchcomms.TorchWork" title="torchcomms.TorchWork"><span class="pre">torchcomms.TorchWork</span></a></span></span><a class="headerlink" href="#torchcomms.TorchComm.all_to_all_v_single" title="Link to this definition">#</a></dt>
<dd><p>All-to-all single tensor operation with variable split sizes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output</strong> – Output tensor.</p></li>
<li><p><strong>input</strong> – Input tensor to split and scatter.</p></li>
<li><p><strong>output_split_sizes</strong> – List of output split sizes.</p></li>
<li><p><strong>input_split_sizes</strong> – List of input split sizes.</p></li>
<li><p><strong>async_op</strong> – Whether to perform the operation asynchronously.</p></li>
<li><p><strong>hints</strong> – Dictionary of string hints for backend-specific options.</p></li>
<li><p><strong>timeout</strong> – Timeout for the operation.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchComm.barrier">
<span class="sig-name descname"><span class="pre">barrier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">torchcomms.TorchComm</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">async_op</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hints</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">collections.abc.Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">datetime.timedelta</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchcomms.TorchWork" title="torchcomms.TorchWork"><span class="pre">torchcomms.TorchWork</span></a></span></span><a class="headerlink" href="#torchcomms.TorchComm.barrier" title="Link to this definition">#</a></dt>
<dd><p>Block until all ranks have reached this call.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>async_op</strong> – Whether to perform the operation asynchronously.</p></li>
<li><p><strong>hints</strong> – Dictionary of string hints for backend-specific options.</p></li>
<li><p><strong>timeout</strong> – Timeout for the operation.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchComm.batch_op_create">
<span class="sig-name descname"><span class="pre">batch_op_create</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">torchcomms.TorchComm</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchcomms.BatchSendRecv" title="torchcomms.BatchSendRecv"><span class="pre">torchcomms.BatchSendRecv</span></a></span></span><a class="headerlink" href="#torchcomms.TorchComm.batch_op_create" title="Link to this definition">#</a></dt>
<dd><p>Create a batch operation object for batched P2P operations.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchComm.broadcast">
<span class="sig-name descname"><span class="pre">broadcast</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">torchcomms.TorchComm</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">root</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">SupportsInt</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">async_op</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hints</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">collections.abc.Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">datetime.timedelta</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchcomms.TorchWork" title="torchcomms.TorchWork"><span class="pre">torchcomms.TorchWork</span></a></span></span><a class="headerlink" href="#torchcomms.TorchComm.broadcast" title="Link to this definition">#</a></dt>
<dd><p>Broadcast tensor to all ranks in the communicator.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> – the tensor to broadcast if root or receive into if not root</p></li>
<li><p><strong>root</strong> – the root rank</p></li>
<li><p><strong>async_op</strong> – whether to perform the operation asynchronously</p></li>
<li><p><strong>hints</strong> – dictionary of string hints for backend-specific options</p></li>
<li><p><strong>timeout</strong> – timeout for the operation</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchComm.finalize">
<span class="sig-name descname"><span class="pre">finalize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">torchcomms.TorchComm</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#torchcomms.TorchComm.finalize" title="Link to this definition">#</a></dt>
<dd><p>Finalize and free all resources. This must be called prior to destruction.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchComm.gather">
<span class="sig-name descname"><span class="pre">gather</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">torchcomms.TorchComm</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_tensor_list</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">collections.abc.Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_tensor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">root</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">SupportsInt</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">async_op</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hints</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">collections.abc.Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">datetime.timedelta</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchcomms.TorchWork" title="torchcomms.TorchWork"><span class="pre">torchcomms.TorchWork</span></a></span></span><a class="headerlink" href="#torchcomms.TorchComm.gather" title="Link to this definition">#</a></dt>
<dd><p>Gather the input tensor from all ranks to the root.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_tensor_list</strong> – Output tensor list. Will be empty on non-root ranks.</p></li>
<li><p><strong>input_tensor</strong> – Input tensor to gather.</p></li>
<li><p><strong>root</strong> – The root rank.</p></li>
<li><p><strong>async_op</strong> – Whether to perform the operation asynchronously.</p></li>
<li><p><strong>hints</strong> – Dictionary of string hints for backend-specific options.</p></li>
<li><p><strong>timeout</strong> – Timeout for the operation.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchComm.get_backend">
<span class="sig-name descname"><span class="pre">get_backend</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">torchcomms.TorchComm</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#torchcomms.TorchComm.get_backend" title="Link to this definition">#</a></dt>
<dd><p>Get communicator backend name</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchComm.get_device">
<span class="sig-name descname"><span class="pre">get_device</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">torchcomms.TorchComm</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.device</span></span></span><a class="headerlink" href="#torchcomms.TorchComm.get_device" title="Link to this definition">#</a></dt>
<dd><p>Get the communicator device</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchComm.get_name">
<span class="sig-name descname"><span class="pre">get_name</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">torchcomms.TorchComm</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#torchcomms.TorchComm.get_name" title="Link to this definition">#</a></dt>
<dd><p>Get the name of the communicator</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchComm.get_options">
<span class="sig-name descname"><span class="pre">get_options</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">torchcomms.TorchComm</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchcomms.CommOptions" title="torchcomms.CommOptions"><span class="pre">torchcomms.CommOptions</span></a></span></span><a class="headerlink" href="#torchcomms.TorchComm.get_options" title="Link to this definition">#</a></dt>
<dd><p>Get the communicator options</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchComm.get_rank">
<span class="sig-name descname"><span class="pre">get_rank</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">torchcomms.TorchComm</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#torchcomms.TorchComm.get_rank" title="Link to this definition">#</a></dt>
<dd><p>Get the rank of this process</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchComm.get_size">
<span class="sig-name descname"><span class="pre">get_size</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">torchcomms.TorchComm</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#torchcomms.TorchComm.get_size" title="Link to this definition">#</a></dt>
<dd><p>Get the world size</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torchcomms.TorchComm.mem_allocator">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mem_allocator</span></span><a class="headerlink" href="#torchcomms.TorchComm.mem_allocator" title="Link to this definition">#</a></dt>
<dd><p>Get the communication-specific memory allocator</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchComm.new_window">
<span class="sig-name descname"><span class="pre">new_window</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">torchcomms.TorchComm</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchcomms.TorchCommWindow" title="torchcomms.TorchCommWindow"><span class="pre">torchcomms.TorchCommWindow</span></a></span></span><a class="headerlink" href="#torchcomms.TorchComm.new_window" title="Link to this definition">#</a></dt>
<dd><p>Create a new window object for RMA operations.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>An unregistered window object. Call window.tensor_register(tensor) to register a tensor buffer.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="#torchcomms.TorchCommWindow" title="torchcomms.TorchCommWindow">TorchCommWindow</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchComm.recv">
<span class="sig-name descname"><span class="pre">recv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">torchcomms.TorchComm</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">SupportsInt</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">async_op</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hints</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">collections.abc.Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">datetime.timedelta</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchcomms.TorchWork" title="torchcomms.TorchWork"><span class="pre">torchcomms.TorchWork</span></a></span></span><a class="headerlink" href="#torchcomms.TorchComm.recv" title="Link to this definition">#</a></dt>
<dd><p>Receive tensor from source rank.</p>
<p>This will not run concurrently with other operations (including send/recv) on
the same stream.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> – the tensor to receive into</p></li>
<li><p><strong>src</strong> – the source rank</p></li>
<li><p><strong>async_op</strong> – whether to perform the operation asynchronously</p></li>
<li><p><strong>hints</strong> – dictionary of string hints for backend-specific options</p></li>
<li><p><strong>timeout</strong> – timeout for the operation</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchComm.reduce">
<span class="sig-name descname"><span class="pre">reduce</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">torchcomms.TorchComm</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">root</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">SupportsInt</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">op</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.ReduceOp" title="torchcomms.ReduceOp"><span class="pre">torchcomms.ReduceOp</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">async_op</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hints</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">collections.abc.Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">datetime.timedelta</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchcomms.TorchWork" title="torchcomms.TorchWork"><span class="pre">torchcomms.TorchWork</span></a></span></span><a class="headerlink" href="#torchcomms.TorchComm.reduce" title="Link to this definition">#</a></dt>
<dd><p>Reduce a tensor from all ranks to a single rank in the communicator.</p>
<p>Output will only be available on the root rank.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> – the tensor to reduce</p></li>
<li><p><strong>root</strong> – the root rank</p></li>
<li><p><strong>op</strong> – the reduction operation</p></li>
<li><p><strong>async_op</strong> – whether to perform the operation asynchronously</p></li>
<li><p><strong>hints</strong> – dictionary of string hints for backend-specific options</p></li>
<li><p><strong>timeout</strong> – timeout for the operation</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchComm.reduce_scatter">
<span class="sig-name descname"><span class="pre">reduce_scatter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">torchcomms.TorchComm</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">output</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_list</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">collections.abc.Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">op</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.ReduceOp" title="torchcomms.ReduceOp"><span class="pre">torchcomms.ReduceOp</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">async_op</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hints</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">collections.abc.Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">datetime.timedelta</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchcomms.TorchWork" title="torchcomms.TorchWork"><span class="pre">torchcomms.TorchWork</span></a></span></span><a class="headerlink" href="#torchcomms.TorchComm.reduce_scatter" title="Link to this definition">#</a></dt>
<dd><p>Reduce, then scatter a list of tensors to all ranks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output</strong> – Output tensor.</p></li>
<li><p><strong>input_list</strong> – List of tensors to reduce and scatter.</p></li>
<li><p><strong>op</strong> – Reduction operation.</p></li>
<li><p><strong>async_op</strong> – Whether to perform the operation asynchronously.</p></li>
<li><p><strong>hints</strong> – Dictionary of string hints for backend-specific options.</p></li>
<li><p><strong>timeout</strong> – Timeout for the operation.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchComm.reduce_scatter_single">
<span class="sig-name descname"><span class="pre">reduce_scatter_single</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">torchcomms.TorchComm</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">output</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">op</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.ReduceOp" title="torchcomms.ReduceOp"><span class="pre">torchcomms.ReduceOp</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">async_op</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hints</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">collections.abc.Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">datetime.timedelta</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchcomms.TorchWork" title="torchcomms.TorchWork"><span class="pre">torchcomms.TorchWork</span></a></span></span><a class="headerlink" href="#torchcomms.TorchComm.reduce_scatter_single" title="Link to this definition">#</a></dt>
<dd><p>Reduce, then scatter a single tensor to all ranks.</p>
<p>The input tensor must be of size (world_size * output.numel()).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output</strong> – Output tensor.</p></li>
<li><p><strong>input</strong> – Input tensor to reduce and scatter.</p></li>
<li><p><strong>op</strong> – Reduction operation.</p></li>
<li><p><strong>async_op</strong> – Whether to perform the operation asynchronously.</p></li>
<li><p><strong>hints</strong> – Dictionary of string hints for backend-specific options.</p></li>
<li><p><strong>timeout</strong> – Timeout for the operation.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchComm.reduce_scatter_v">
<span class="sig-name descname"><span class="pre">reduce_scatter_v</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">torchcomms.TorchComm</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">output</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_list</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">collections.abc.Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">op</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.ReduceOp" title="torchcomms.ReduceOp"><span class="pre">torchcomms.ReduceOp</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">async_op</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hints</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">collections.abc.Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">datetime.timedelta</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchcomms.TorchWork" title="torchcomms.TorchWork"><span class="pre">torchcomms.TorchWork</span></a></span></span><a class="headerlink" href="#torchcomms.TorchComm.reduce_scatter_v" title="Link to this definition">#</a></dt>
<dd><p>Reduce, then scatter a list of tensors to all ranks, supporting variable tensor sizes per rank.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output</strong> – Output tensor on each rank; size may differ per rank.</p></li>
<li><p><strong>input_list</strong> – List of tensors to reduce and scatter; the list is the same on all ranks, but tensor sizes may differ between indices.</p></li>
<li><p><strong>op</strong> – Reduction operation.</p></li>
<li><p><strong>async_op</strong> – Whether to perform the operation asynchronously</p></li>
<li><p><strong>hints</strong> – Dictionary of string hints for backend-specific options.</p></li>
<li><p><strong>timeout</strong> – Timeout for the operation.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchComm.scatter">
<span class="sig-name descname"><span class="pre">scatter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">torchcomms.TorchComm</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_tensor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_tensor_list</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">collections.abc.Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">root</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">SupportsInt</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">async_op</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hints</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">collections.abc.Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">datetime.timedelta</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchcomms.TorchWork" title="torchcomms.TorchWork"><span class="pre">torchcomms.TorchWork</span></a></span></span><a class="headerlink" href="#torchcomms.TorchComm.scatter" title="Link to this definition">#</a></dt>
<dd><p>Scatter the split list to all ranks from the root.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_tensor</strong> – Output tensor.</p></li>
<li><p><strong>input_tensor_list</strong> – Input tensor list to scatter.</p></li>
<li><p><strong>root</strong> – The root rank.</p></li>
<li><p><strong>async_op</strong> – Whether to perform the operation asynchronously.</p></li>
<li><p><strong>hints</strong> – Dictionary of string hints for backend-specific options.</p></li>
<li><p><strong>timeout</strong> – Timeout for the operation.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchComm.send">
<span class="sig-name descname"><span class="pre">send</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">torchcomms.TorchComm</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dst</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">SupportsInt</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">async_op</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hints</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">collections.abc.Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">datetime.timedelta</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchcomms.TorchWork" title="torchcomms.TorchWork"><span class="pre">torchcomms.TorchWork</span></a></span></span><a class="headerlink" href="#torchcomms.TorchComm.send" title="Link to this definition">#</a></dt>
<dd><p>Send tensor to destination rank.</p>
<p>This will not run concurrently with other operations (including send/recv) on
the same stream.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> – the tensor to send</p></li>
<li><p><strong>dst</strong> – the destination rank</p></li>
<li><p><strong>async_op</strong> – whether to perform the operation asynchronously</p></li>
<li><p><strong>hints</strong> – dictionary of string hints for backend-specific options</p></li>
<li><p><strong>timeout</strong> – timeout for the operation</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchComm.split">
<span class="sig-name descname"><span class="pre">split</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">torchcomms.TorchComm</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">ranks</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">collections.abc.Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">SupportsInt</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hints</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">collections.abc.Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">datetime.timedelta</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">torchcomms.TorchComm</span></a></span></span><a class="headerlink" href="#torchcomms.TorchComm.split" title="Link to this definition">#</a></dt>
<dd><p>Split communicator into a subgroup.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ranks</strong> – List of ranks to include in the new subgroup. If the list is empty,
None will be returned. If the list is non-empty but does not include
the current rank, an exception will be thrown.</p></li>
<li><p><strong>name</strong> – Name for the new communicator.</p></li>
<li><p><strong>hints</strong> – Dictionary of string hints for backend-specific options.</p></li>
<li><p><strong>timeout</strong> – Timeout for the operation.</p></li>
</ul>
</dd>
</dl>
<p>Returns: A new communicator for the subgroup, or None if the ranks list is empty.</p>
<p>Raises: RuntimeError if the ranks list is non-empty and the current rank is not included.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchComm.unsafe_get_backend">
<span class="sig-name descname"><span class="pre">unsafe_get_backend</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">torchcomms.TorchComm</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torchcomms.TorchCommBackend</span></span></span><a class="headerlink" href="#torchcomms.TorchComm.unsafe_get_backend" title="Link to this definition">#</a></dt>
<dd><p>Get communicator backend implementation.</p>
<p>WARNING: This is intended as an escape hatch for experimentation and
development. Direct backend access provides no backwards compatibility
guarantees. Users depending on unsafe_get_backend should expect their code to
break as interfaces change.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchcomms.ReduceOp">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchcomms.</span></span><span class="sig-name descname"><span class="pre">ReduceOp</span></span><a class="headerlink" href="#torchcomms.ReduceOp" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">pybind11_object</span></code></p>
<p>Operation to perform during reduction.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="torchcomms.ReduceOp.AVG">
<span class="sig-name descname"><span class="pre">AVG</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">&lt;torchcomms.ReduceOp</span> <span class="pre">object&gt;</span></em><a class="headerlink" href="#torchcomms.ReduceOp.AVG" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torchcomms.ReduceOp.BAND">
<span class="sig-name descname"><span class="pre">BAND</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">&lt;torchcomms.ReduceOp</span> <span class="pre">object&gt;</span></em><a class="headerlink" href="#torchcomms.ReduceOp.BAND" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torchcomms.ReduceOp.BOR">
<span class="sig-name descname"><span class="pre">BOR</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">&lt;torchcomms.ReduceOp</span> <span class="pre">object&gt;</span></em><a class="headerlink" href="#torchcomms.ReduceOp.BOR" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torchcomms.ReduceOp.BXOR">
<span class="sig-name descname"><span class="pre">BXOR</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">&lt;torchcomms.ReduceOp</span> <span class="pre">object&gt;</span></em><a class="headerlink" href="#torchcomms.ReduceOp.BXOR" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torchcomms.ReduceOp.MAX">
<span class="sig-name descname"><span class="pre">MAX</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">&lt;torchcomms.ReduceOp</span> <span class="pre">object&gt;</span></em><a class="headerlink" href="#torchcomms.ReduceOp.MAX" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torchcomms.ReduceOp.MIN">
<span class="sig-name descname"><span class="pre">MIN</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">&lt;torchcomms.ReduceOp</span> <span class="pre">object&gt;</span></em><a class="headerlink" href="#torchcomms.ReduceOp.MIN" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.ReduceOp.PREMUL_SUM">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">PREMUL_SUM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">factor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">SupportsFloat</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchcomms.ReduceOp" title="torchcomms.ReduceOp"><span class="pre">torchcomms.ReduceOp</span></a></span></span><a class="headerlink" href="#torchcomms.ReduceOp.PREMUL_SUM" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torchcomms.ReduceOp.PRODUCT">
<span class="sig-name descname"><span class="pre">PRODUCT</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">&lt;torchcomms.ReduceOp</span> <span class="pre">object&gt;</span></em><a class="headerlink" href="#torchcomms.ReduceOp.PRODUCT" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torchcomms.ReduceOp.SUM">
<span class="sig-name descname"><span class="pre">SUM</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">&lt;torchcomms.ReduceOp</span> <span class="pre">object&gt;</span></em><a class="headerlink" href="#torchcomms.ReduceOp.SUM" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.ReduceOp.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.ReduceOp" title="torchcomms.ReduceOp"><span class="pre">torchcomms.ReduceOp</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">opType</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torchcomms.RedOpType</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#torchcomms.ReduceOp.__init__" title="Link to this definition">#</a></dt>
<dd><p>Create default ReduceOp</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torchcomms.ReduceOp.type">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">type</span></span><a class="headerlink" href="#torchcomms.ReduceOp.type" title="Link to this definition">#</a></dt>
<dd><p>Get the type of the operation</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchcomms.TorchWork">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchcomms.</span></span><span class="sig-name descname"><span class="pre">TorchWork</span></span><a class="headerlink" href="#torchcomms.TorchWork" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">pybind11_object</span></code></p>
<p>TorchWork allows you to track whether an asynchronous operation has completed.</p>
<p>When async_op=True, the operation is enqueued on a background stream and a
TorchWork object is returned. This work object must be waited on before
using the output tensor.</p>
<p>This is intended to make it easier to write efficient code that can overlap
communication with computation.</p>
<p>Example usage:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span> <span class="o">=</span> <span class="o">...</span>
<span class="c1"># run all_reduce on a background stream and return a TorchWork object</span>
<span class="n">work</span> <span class="o">=</span> <span class="n">torchcomms</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Schedule some other work on the current stream</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">b</span> <span class="o">*</span> <span class="mi">2</span>

<span class="c1"># block the current stream until the all_reduce is complete</span>
<span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>

<span class="c1"># safely use the tensor after the collective completes</span>
<span class="n">tensor</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="c1"># block CPU until stream is complete</span>
<span class="n">torch</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchWork.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchcomms.TorchWork.__init__" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchWork.is_completed">
<span class="sig-name descname"><span class="pre">is_completed</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchWork" title="torchcomms.TorchWork"><span class="pre">torchcomms.TorchWork</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#torchcomms.TorchWork.is_completed" title="Link to this definition">#</a></dt>
<dd><p>Check if the work is completed</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchWork.wait">
<span class="sig-name descname"><span class="pre">wait</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchWork" title="torchcomms.TorchWork"><span class="pre">torchcomms.TorchWork</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#torchcomms.TorchWork.wait" title="Link to this definition">#</a></dt>
<dd><p>Block the current stream until the work is completed.</p>
<p>See <a class="reference external" href="https://docs.pytorch.org/docs/stable/notes/cuda.html#cuda-streams">https://docs.pytorch.org/docs/stable/notes/cuda.html#cuda-streams</a> for more details.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchcomms.BatchP2POptions">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchcomms.</span></span><span class="sig-name descname"><span class="pre">BatchP2POptions</span></span><a class="headerlink" href="#torchcomms.BatchP2POptions" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">pybind11_object</span></code></p>
<p>Options for batched P2P operations.</p>
<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.BatchP2POptions.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.BatchP2POptions" title="torchcomms.BatchP2POptions"><span class="pre">torchcomms.BatchP2POptions</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#torchcomms.BatchP2POptions.__init__" title="Link to this definition">#</a></dt>
<dd><p>Create default BatchP2POptions</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torchcomms.BatchP2POptions.hints">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">hints</span></span><a class="headerlink" href="#torchcomms.BatchP2POptions.hints" title="Link to this definition">#</a></dt>
<dd><p>Hints dictionary</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torchcomms.BatchP2POptions.timeout">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">timeout</span></span><a class="headerlink" href="#torchcomms.BatchP2POptions.timeout" title="Link to this definition">#</a></dt>
<dd><p>Timeout</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchcomms.BatchSendRecv">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchcomms.</span></span><span class="sig-name descname"><span class="pre">BatchSendRecv</span></span><a class="headerlink" href="#torchcomms.BatchSendRecv" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">pybind11_object</span></code></p>
<p>BatchSendRecv allows you to run multiple send/recv operations concurrently
unlike the standard send/recv APIs which only allow you to have one inflight at
a time.</p>
<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.BatchSendRecv.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchcomms.BatchSendRecv.__init__" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.BatchSendRecv.issue">
<span class="sig-name descname"><span class="pre">issue</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self:</span> <span class="pre">torchcomms.BatchSendRecv</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">async_op:</span> <span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">options:</span> <span class="pre">torchcomms.BatchP2POptions</span> <span class="pre">=</span> <span class="pre">&lt;torchcomms.BatchP2POptions</span> <span class="pre">object</span> <span class="pre">at</span> <span class="pre">0x7f1498037ef0&gt;</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchcomms.TorchWork" title="torchcomms.TorchWork"><span class="pre">torchcomms.TorchWork</span></a></span></span><a class="headerlink" href="#torchcomms.BatchSendRecv.issue" title="Link to this definition">#</a></dt>
<dd><p>Issues the batched operations</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torchcomms.BatchSendRecv.ops">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">ops</span></span><a class="headerlink" href="#torchcomms.BatchSendRecv.ops" title="Link to this definition">#</a></dt>
<dd><p>List of P2P operations</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.BatchSendRecv.recv">
<span class="sig-name descname"><span class="pre">recv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.BatchSendRecv" title="torchcomms.BatchSendRecv"><span class="pre">torchcomms.BatchSendRecv</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">SupportsInt</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#torchcomms.BatchSendRecv.recv" title="Link to this definition">#</a></dt>
<dd><p>Add recv operation to batch. Must be paired with a corresponding send operation
on a different rank.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> – the tensor to receive into</p></li>
<li><p><strong>src</strong> – the source rank</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.BatchSendRecv.send">
<span class="sig-name descname"><span class="pre">send</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.BatchSendRecv" title="torchcomms.BatchSendRecv"><span class="pre">torchcomms.BatchSendRecv</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dst</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">SupportsInt</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#torchcomms.BatchSendRecv.send" title="Link to this definition">#</a></dt>
<dd><p>Add send operation to batch. Must be paired with a corresponding
recv operation on a different rank.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> – the tensor to send</p></li>
<li><p><strong>dst</strong> – the destination rank</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchcomms.P2POp">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchcomms.</span></span><span class="sig-name descname"><span class="pre">P2POp</span></span><a class="headerlink" href="#torchcomms.P2POp" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">pybind11_object</span></code></p>
<p>Represents a peer to peer operation as part of a batch.</p>
<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.P2POp.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self:</span> <span class="pre">torchcomms.P2POp</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">type:</span> <span class="pre">torch::comms::BatchSendRecv::P2POp::OpType</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor:</span> <span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">peer:</span> <span class="pre">typing.SupportsInt</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#torchcomms.P2POp.__init__" title="Link to this definition">#</a></dt>
<dd><p>Create P2POp.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>type</strong> – the type of the operations i.e. send/recv</p></li>
<li><p><strong>tensor</strong> – the tensor to operate on</p></li>
<li><p><strong>peer</strong> – the rank of the peer</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torchcomms.P2POp.peer">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">peer</span></span><a class="headerlink" href="#torchcomms.P2POp.peer" title="Link to this definition">#</a></dt>
<dd><p>Peer rank</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torchcomms.P2POp.tensor">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">tensor</span></span><a class="headerlink" href="#torchcomms.P2POp.tensor" title="Link to this definition">#</a></dt>
<dd><p>Tensor</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torchcomms.P2POp.type">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">type</span></span><a class="headerlink" href="#torchcomms.P2POp.type" title="Link to this definition">#</a></dt>
<dd><p>Operation type</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchcomms.CommOptions">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchcomms.</span></span><span class="sig-name descname"><span class="pre">CommOptions</span></span><a class="headerlink" href="#torchcomms.CommOptions" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">pybind11_object</span></code></p>
<p>Options for communicator creation.</p>
<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.CommOptions.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.CommOptions" title="torchcomms.CommOptions"><span class="pre">torchcomms.CommOptions</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#torchcomms.CommOptions.__init__" title="Link to this definition">#</a></dt>
<dd><p>Create default CommOptions</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torchcomms.CommOptions.abort_process_on_timeout_or_error">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">abort_process_on_timeout_or_error</span></span><a class="headerlink" href="#torchcomms.CommOptions.abort_process_on_timeout_or_error" title="Link to this definition">#</a></dt>
<dd><p>Whether to abort process on timeout or error</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torchcomms.CommOptions.hints">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">hints</span></span><a class="headerlink" href="#torchcomms.CommOptions.hints" title="Link to this definition">#</a></dt>
<dd><p>Dictionary of string hints for backend-specific options</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torchcomms.CommOptions.store">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">store</span></span><a class="headerlink" href="#torchcomms.CommOptions.store" title="Link to this definition">#</a></dt>
<dd><p>Store for communication between processes</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torchcomms.CommOptions.timeout">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">timeout</span></span><a class="headerlink" href="#torchcomms.CommOptions.timeout" title="Link to this definition">#</a></dt>
<dd><p>Timeout for operations (milliseconds)</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchcomms.TorchCommWindow">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchcomms.</span></span><span class="sig-name descname"><span class="pre">TorchCommWindow</span></span><a class="headerlink" href="#torchcomms.TorchCommWindow" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">pybind11_object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchCommWindow.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchcomms.TorchCommWindow.__init__" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchCommWindow.get_attr">
<span class="sig-name descname"><span class="pre">get_attr</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchCommWindow" title="torchcomms.TorchCommWindow"><span class="pre">torchcomms.TorchCommWindow</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">peer_rank</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">SupportsInt</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torchcomms.TorchCommWindowAttr</span></span></span><a class="headerlink" href="#torchcomms.TorchCommWindow.get_attr" title="Link to this definition">#</a></dt>
<dd><p>get the attribute of the window</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchCommWindow.get_size">
<span class="sig-name descname"><span class="pre">get_size</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchCommWindow" title="torchcomms.TorchCommWindow"><span class="pre">torchcomms.TorchCommWindow</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#torchcomms.TorchCommWindow.get_size" title="Link to this definition">#</a></dt>
<dd><p>Get the size of the window</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchCommWindow.map_remote_tensor">
<span class="sig-name descname"><span class="pre">map_remote_tensor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchCommWindow" title="torchcomms.TorchCommWindow"><span class="pre">torchcomms.TorchCommWindow</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">SupportsInt</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="headerlink" href="#torchcomms.TorchCommWindow.map_remote_tensor" title="Link to this definition">#</a></dt>
<dd><p>Get the entire tensor view from the remote rank’s window buffer.</p>
<p>This method returns a tensor view of the complete registered buffer from
the specified rank’s window. Users can slice the returned tensor as needed.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>rank</strong> – The rank whose window to access.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tensor view with the same shape as the registered buffer.</p>
</dd>
</dl>
<p class="rubric">Example</p>
<p>If the registered buffer has shape [100, 512, 128]:
- full_tensor = map_remote_tensor(rank=0)</p>
<blockquote>
<div><p>returns a tensor with shape [100, 512, 128]</p>
</div></blockquote>
<ul class="simple">
<li><p>You can then slice it: sliced = full_tensor[20:65]</p></li>
</ul>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchCommWindow.put">
<span class="sig-name descname"><span class="pre">put</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchCommWindow" title="torchcomms.TorchCommWindow"><span class="pre">torchcomms.TorchCommWindow</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dst_rank</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">SupportsInt</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_offset_nelems</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">SupportsInt</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">async_op</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hints</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">collections.abc.Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">datetime.timedelta</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchcomms.TorchWork" title="torchcomms.TorchWork"><span class="pre">torchcomms.TorchWork</span></a></span></span><a class="headerlink" href="#torchcomms.TorchCommWindow.put" title="Link to this definition">#</a></dt>
<dd><p>Put allows you to put a tensor into the previously allocated remote window.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> – the tensor to put</p></li>
<li><p><strong>dst_rank</strong> – the destination rank</p></li>
<li><p><strong>target_offset_nelems</strong> – the target offset in number of elements</p></li>
<li><p><strong>async_op</strong> – if this is true, the operation is asynced and will be enqueued on a background stream and a TorchWork object is returned.</p></li>
</ul>
</dd>
</dl>
<p>Example usage:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span> <span class="o">=</span> <span class="o">...</span>
<span class="c1"># create a window</span>
<span class="n">window</span> <span class="o">=</span> <span class="n">torchcomms</span><span class="o">.</span><span class="n">create_window</span><span class="p">(</span><span class="n">window_size</span><span class="p">,</span> <span class="n">cpu_buf</span><span class="p">)</span>
<span class="c1"># put a tensor into the window</span>
<span class="n">work</span> <span class="o">=</span> <span class="n">window</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dst_rank</span><span class="p">,</span> <span class="n">target_offset_nelems</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>

<span class="c1"># on the remote side, get the tensor from the window after waiting on the remote signal</span>
<span class="n">tensor</span> <span class="o">=</span> <span class="n">window</span><span class="o">.</span><span class="n">map_remote_tensor</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>

<span class="c1"># safely use the tensor after the collective completes</span>
<span class="n">tensor</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchCommWindow.signal">
<span class="sig-name descname"><span class="pre">signal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchCommWindow" title="torchcomms.TorchCommWindow"><span class="pre">torchcomms.TorchCommWindow</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">peer_rank</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">SupportsInt</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">async_op</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hints</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">collections.abc.Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">datetime.timedelta</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchcomms.TorchWork" title="torchcomms.TorchWork"><span class="pre">torchcomms.TorchWork</span></a></span></span><a class="headerlink" href="#torchcomms.TorchCommWindow.signal" title="Link to this definition">#</a></dt>
<dd><p>Atomic signal to notify remote peer of a change in state.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>peer_rank</strong> – the rank of the remote peer to signal.</p></li>
<li><p><strong>async_op</strong> – if this is true, the operation is asynced.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchCommWindow.tensor_deregister">
<span class="sig-name descname"><span class="pre">tensor_deregister</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchCommWindow" title="torchcomms.TorchCommWindow"><span class="pre">torchcomms.TorchCommWindow</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#torchcomms.TorchCommWindow.tensor_deregister" title="Link to this definition">#</a></dt>
<dd><p>Deregister the window and free all associated resources.</p>
<p>This is a collective operation that includes internal barriers to ensure:
1. All ranks have finished using the window before deregistration
2. All ranks have completed deregistration before proceeding</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchCommWindow.tensor_register">
<span class="sig-name descname"><span class="pre">tensor_register</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchCommWindow" title="torchcomms.TorchCommWindow"><span class="pre">torchcomms.TorchCommWindow</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#torchcomms.TorchCommWindow.tensor_register" title="Link to this definition">#</a></dt>
<dd><p>Register a tensor buffer to a window for RMA operations.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>tensor</strong> – the contiguous tensor buffer to register as a window</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchcomms.TorchCommWindow.wait_signal">
<span class="sig-name descname"><span class="pre">wait_signal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchCommWindow" title="torchcomms.TorchCommWindow"><span class="pre">torchcomms.TorchCommWindow</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">peer_rank</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">SupportsInt</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">async_op</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hints</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">collections.abc.Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">datetime.timedelta</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchcomms.TorchWork" title="torchcomms.TorchWork"><span class="pre">torchcomms.TorchWork</span></a></span></span><a class="headerlink" href="#torchcomms.TorchCommWindow.wait_signal" title="Link to this definition">#</a></dt>
<dd><p>wait for a signal from remote peer</p>
</dd></dl>

</dd></dl>

<dl class="py function" id="module-torchcomms.device_mesh">
<dt class="sig sig-object py" id="torchcomms.device_mesh.init_device_mesh">
<span class="sig-prename descclassname"><span class="pre">torchcomms.device_mesh.</span></span><span class="sig-name descname"><span class="pre">init_device_mesh</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mesh_dim_comms</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">TorchComm</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mesh_dim_names</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_global_comm</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">TorchComm</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">DeviceMesh</span></span></span><a class="reference internal" href="_modules/torchcomms/device_mesh.html#init_device_mesh"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchcomms.device_mesh.init_device_mesh" title="Link to this definition">#</a></dt>
<dd><p>Initializes a <cite>DeviceMesh</cite> from the list of provided <cite>TorchComm</cite> instances.</p>
<p>See <cite>DeviceMesh</cite> for more details.</p>
</dd></dl>

<dl class="py function" id="module-torchcomms.objcol">
<dt class="sig sig-object py" id="torchcomms.objcol.get_serialization">
<span class="sig-prename descclassname"><span class="pre">torchcomms.objcol.</span></span><span class="sig-name descname"><span class="pre">get_serialization</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_Serialization</span></span></span><a class="reference internal" href="_modules/torchcomms/objcol.html#get_serialization"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchcomms.objcol.get_serialization" title="Link to this definition">#</a></dt>
<dd><p>Returns a cached serialization object with serialize and deserialize methods.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torchcomms.objcol.all_gather_object">
<span class="sig-prename descclassname"><span class="pre">torchcomms.objcol.</span></span><span class="sig-name descname"><span class="pre">all_gather_object</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">comm</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">TorchComm</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">object_list</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">obj</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">object</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">timedelta</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights_only</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/torchcomms/objcol.html#all_gather_object"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchcomms.objcol.all_gather_object" title="Link to this definition">#</a></dt>
<dd><p>Gathers picklable objects from the whole comm into a list.</p>
<p>Similar to <code class="xref py py-func docutils literal notranslate"><span class="pre">all_gather()</span></code>, but Python objects can be passed in.
Note that the object must be picklable in order to be gathered.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>comm</strong> – The comm to work on.</p></li>
<li><p><strong>object_list</strong> (<em>list</em><em>[</em><em>object</em><em>]</em>) – Output list. It should be correctly sized as the
size of the comm for this collective and will contain the output.</p></li>
<li><p><strong>obj</strong> (<em>object</em>) – Pickable Python object to be broadcast from current process.</p></li>
<li><p><strong>timeout</strong> – (timedelta, optional): Timeout for collective operations. If
<code class="docutils literal notranslate"><span class="pre">None</span></code>, will use the default timeout for the backend.</p></li>
<li><p><strong>weights_only</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, only safe objects such as
weights are allowed to be deserialized.
<a class="reference external" href="https://docs.pytorch.org/docs/stable/notes/serialization.html#weights-only">https://docs.pytorch.org/docs/stable/notes/serialization.html#weights-only</a></p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>None. If the calling rank is part of this comm, the output of the
collective will be populated into the input <code class="docutils literal notranslate"><span class="pre">object_list</span></code>. If the
calling rank is not part of the comm, the passed in <code class="docutils literal notranslate"><span class="pre">object_list</span></code> will
be unmodified.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Note that this API differs slightly from the <code class="xref py py-func docutils literal notranslate"><span class="pre">all_gather()</span></code>
collective since it does not provide an <code class="docutils literal notranslate"><span class="pre">async_op</span></code> handle and thus
will be a blocking call.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For NCCL-based processed comms, internal tensor representations
of objects must be moved to the GPU device before communication takes
place. In this case, the device used is given by
<code class="docutils literal notranslate"><span class="pre">torch.cuda.current_device()</span></code> and it is the user’s responsibility to
ensure that this is set so that each rank has an individual GPU, via
<code class="docutils literal notranslate"><span class="pre">torch.cuda.set_device()</span></code>.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Object collectives have a number of serious performance and scalability
limitations.  See <span class="xref std std-ref">object_collectives</span> for details.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><a class="reference internal" href="#torchcomms.objcol.all_gather_object" title="torchcomms.objcol.all_gather_object"><code class="xref py py-func docutils literal notranslate"><span class="pre">all_gather_object()</span></code></a> uses <code class="docutils literal notranslate"><span class="pre">pickle</span></code> module implicitly, which is
known to be insecure. It is possible to construct malicious pickle data
which will execute arbitrary code during unpickling. Only call this
function with data you trust.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Calling <a class="reference internal" href="#torchcomms.objcol.all_gather_object" title="torchcomms.objcol.all_gather_object"><code class="xref py py-func docutils literal notranslate"><span class="pre">all_gather_object()</span></code></a> with GPU tensors is not well supported
and inefficient as it incurs GPU -&gt; CPU transfer since tensors would be
pickled. Please consider using <code class="xref py py-func docutils literal notranslate"><span class="pre">all_gather()</span></code> instead.</p>
</div>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP(&quot;need comm init&quot;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Note: comm initialization omitted on each rank.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchcomms</span><span class="w"> </span><span class="kn">import</span> <span class="n">objcol</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Assumes world_size of 3.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gather_objects</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;foo&quot;</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="p">{</span><span class="mi">1</span><span class="p">:</span> <span class="mi">2</span><span class="p">}]</span> <span class="c1"># any picklable object</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">gather_objects</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">objcol</span><span class="o">.</span><span class="n">all_gather_object</span><span class="p">(</span><span class="n">comm</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">gather_objects</span><span class="p">[</span><span class="n">comm</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">[&#39;foo&#39;, 12, {1: 2}]</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torchcomms.objcol.gather_object">
<span class="sig-prename descclassname"><span class="pre">torchcomms.objcol.</span></span><span class="sig-name descname"><span class="pre">gather_object</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">comm</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">TorchComm</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">obj</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">object</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">root</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">object_gather_list</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">timedelta</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights_only</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/torchcomms/objcol.html#gather_object"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchcomms.objcol.gather_object" title="Link to this definition">#</a></dt>
<dd><p>Gathers picklable objects from the whole comm in a single process.</p>
<p>Similar to <code class="xref py py-func docutils literal notranslate"><span class="pre">gather()</span></code>, but Python objects can be passed in. Note that the
object must be picklable in order to be gathered.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>comm</strong> – The comm to work on.</p></li>
<li><p><strong>obj</strong> (<em>object</em>) – Input object. Must be picklable.</p></li>
<li><p><strong>object_gather_list</strong> (<em>list</em><em>[</em><em>object</em><em>]</em>) – Output list. On the <code class="docutils literal notranslate"><span class="pre">root</span></code> rank, it
should be correctly sized as the size of the comm for this
collective and will contain the output. Must be <code class="docutils literal notranslate"><span class="pre">None</span></code> on non-root
ranks. (default is <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p></li>
<li><p><strong>root</strong> (<em>int</em><em>, </em><em>optional</em>) – Destination rank on <code class="docutils literal notranslate"><span class="pre">comm</span></code>.  Invalid to specify both <code class="docutils literal notranslate"><span class="pre">root</span></code> and <code class="docutils literal notranslate"><span class="pre">root</span></code></p></li>
<li><p><strong>timeout</strong> – (timedelta, optional): Timeout for collective operations. If
<code class="docutils literal notranslate"><span class="pre">None</span></code>, will use the default timeout for the backend.</p></li>
<li><p><strong>weights_only</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, only safe objects such as
weights are allowed to be deserialized.
<a class="reference external" href="https://docs.pytorch.org/docs/stable/notes/serialization.html#weights-only">https://docs.pytorch.org/docs/stable/notes/serialization.html#weights-only</a></p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>None. On the <code class="docutils literal notranslate"><span class="pre">root</span></code> rank, <code class="docutils literal notranslate"><span class="pre">object_gather_list</span></code> will contain the
output of the collective.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Note that this API differs slightly from the gather collective
since it does not provide an async_op handle and thus will be a blocking
call.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For NCCL-based processed comms, internal tensor representations
of objects must be moved to the GPU device before communication takes
place. In this case, the device used is given by
<code class="docutils literal notranslate"><span class="pre">torch.cuda.current_device()</span></code> and it is the user’s responsibility to
ensure that this is set so that each rank has an individual GPU, via
<code class="docutils literal notranslate"><span class="pre">torch.cuda.set_device()</span></code>.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Object collectives have a number of serious performance and scalability
limitations.  See <span class="xref std std-ref">object_collectives</span> for details.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><a class="reference internal" href="#torchcomms.objcol.gather_object" title="torchcomms.objcol.gather_object"><code class="xref py py-func docutils literal notranslate"><span class="pre">gather_object()</span></code></a> uses <code class="docutils literal notranslate"><span class="pre">pickle</span></code> module implicitly, which is
known to be insecure. It is possible to construct malicious pickle data
which will execute arbitrary code during unpickling. Only call this
function with data you trust.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Calling <a class="reference internal" href="#torchcomms.objcol.gather_object" title="torchcomms.objcol.gather_object"><code class="xref py py-func docutils literal notranslate"><span class="pre">gather_object()</span></code></a> with GPU tensors is not well supported
and inefficient as it incurs GPU -&gt; CPU transfer since tensors would be
pickled. Please consider using <code class="xref py py-func docutils literal notranslate"><span class="pre">gather()</span></code> instead.</p>
</div>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP(&quot;need comm init&quot;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Note: comm initialization omitted on each rank.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchcomms</span><span class="w"> </span><span class="kn">import</span> <span class="n">objcol</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Assumes world_size of 3.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gather_objects</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;foo&quot;</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="p">{</span><span class="mi">1</span><span class="p">:</span> <span class="mi">2</span><span class="p">}]</span> <span class="c1"># any picklable object</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">gather_objects</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">objcol</span><span class="o">.</span><span class="n">gather_object</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">comm</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">gather_objects</span><span class="p">[</span><span class="n">comm</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()],</span>
<span class="gp">... </span>    <span class="n">output</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">root</span><span class="o">=</span><span class="mi">0</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># On rank 0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">[&#39;foo&#39;, 12, {1: 2}]</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torchcomms.objcol.send_object_list">
<span class="sig-prename descclassname"><span class="pre">torchcomms.objcol.</span></span><span class="sig-name descname"><span class="pre">send_object_list</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">comm</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">TorchComm</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">object_list</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dst</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">timedelta</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/torchcomms/objcol.html#send_object_list"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchcomms.objcol.send_object_list" title="Link to this definition">#</a></dt>
<dd><p>Sends picklable objects in <code class="docutils literal notranslate"><span class="pre">object_list</span></code> synchronously.</p>
<p>Similar to <code class="xref py py-func docutils literal notranslate"><span class="pre">send()</span></code>, but Python objects can be passed in.
Note that all objects in <code class="docutils literal notranslate"><span class="pre">object_list</span></code> must be picklable in order to be
sent.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>comm</strong> – The comm to work on.</p></li>
<li><p><strong>object_list</strong> (<em>List</em><em>[</em><em>object</em><em>]</em>) – List of input objects to sent.
Each object must be picklable. Receiver must provide lists of equal sizes.</p></li>
<li><p><strong>dst</strong> (<em>int</em>) – Destination rank to send <code class="docutils literal notranslate"><span class="pre">object_list</span></code> to.</p></li>
<li><p><strong>timeout</strong> – (timedelta, optional): Timeout for collective operations. If
<code class="docutils literal notranslate"><span class="pre">None</span></code>, will use the default timeout for the backend.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For NCCL-based comms, internal tensor representations
of objects must be moved to the GPU device before communication takes
place. In this case, the device used is given by
<code class="docutils literal notranslate"><span class="pre">torch.cuda.current_device()</span></code> and it is the user’s responsibility to
ensure that this is set so that each rank has an individual GPU, via
<code class="docutils literal notranslate"><span class="pre">torch.cuda.set_device()</span></code>.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Object collectives have a number of serious performance and scalability
limitations.  See <span class="xref std std-ref">object_collectives</span> for details.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><a class="reference internal" href="#torchcomms.objcol.send_object_list" title="torchcomms.objcol.send_object_list"><code class="xref py py-func docutils literal notranslate"><span class="pre">send_object_list()</span></code></a> uses <code class="docutils literal notranslate"><span class="pre">pickle</span></code> module implicitly, which
is known to be insecure. It is possible to construct malicious pickle
data which will execute arbitrary code during unpickling. Only call this
function with data you trust.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Calling <a class="reference internal" href="#torchcomms.objcol.send_object_list" title="torchcomms.objcol.send_object_list"><code class="xref py py-func docutils literal notranslate"><span class="pre">send_object_list()</span></code></a> with GPU tensors is not well supported
and inefficient as it incurs GPU -&gt; CPU transfer since tensors would be
pickled. Please consider using <code class="xref py py-func docutils literal notranslate"><span class="pre">send()</span></code> instead.</p>
</div>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP(&quot;need comm init&quot;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Note: comm initialization omitted on each rank.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchcomms</span><span class="w"> </span><span class="kn">import</span> <span class="n">objcol</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Assumes backend is not NCCL</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">if</span> <span class="n">comm</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="c1"># Assumes world_size of 2.</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">objects</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;foo&quot;</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="p">{</span><span class="mi">1</span><span class="p">:</span> <span class="mi">2</span><span class="p">}]</span> <span class="c1"># any picklable object</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">objcol</span><span class="o">.</span><span class="n">send_object_list</span><span class="p">(</span><span class="n">comm</span><span class="p">,</span> <span class="n">objects</span><span class="p">,</span> <span class="n">dst</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">else</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">objects</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">objcol</span><span class="o">.</span><span class="n">recv_object_list</span><span class="p">(</span><span class="n">comm</span><span class="p">,</span> <span class="n">objects</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">objects</span>
<span class="go">[&#39;foo&#39;, 12, {1: 2}]</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torchcomms.objcol.recv_object_list">
<span class="sig-prename descclassname"><span class="pre">torchcomms.objcol.</span></span><span class="sig-name descname"><span class="pre">recv_object_list</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">comm</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">TorchComm</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">object_list</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">timedelta</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights_only</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/torchcomms/objcol.html#recv_object_list"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchcomms.objcol.recv_object_list" title="Link to this definition">#</a></dt>
<dd><p>Receives picklable objects in <code class="docutils literal notranslate"><span class="pre">object_list</span></code> synchronously.</p>
<p>Similar to <code class="xref py py-func docutils literal notranslate"><span class="pre">recv()</span></code>, but can receive Python objects.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>comm</strong> – The comm to work on.</p></li>
<li><p><strong>object_list</strong> (<em>List</em><em>[</em><em>object</em><em>]</em>) – List of objects to receive into.
Must provide a list of sizes equal to the size of the list being sent.</p></li>
<li><p><strong>src</strong> (<em>int</em>) – Source rank from which to recv <code class="docutils literal notranslate"><span class="pre">object_list</span></code>.</p></li>
<li><p><strong>timeout</strong> – (timedelta, optional): Timeout for collective operations. If
<code class="docutils literal notranslate"><span class="pre">None</span></code>, will use the default timeout for the backend.</p></li>
<li><p><strong>weights_only</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, only safe objects such as
weights are allowed to be deserialized.
<a class="reference external" href="https://docs.pytorch.org/docs/stable/notes/serialization.html#weights-only">https://docs.pytorch.org/docs/stable/notes/serialization.html#weights-only</a></p></li>
</ul>
</dd>
</dl>
<p>Returns: None</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For NCCL-based comms, internal tensor representations
of objects must be moved to the GPU device before communication takes
place. In this case, the device used is given by
<code class="docutils literal notranslate"><span class="pre">torch.cuda.current_device()</span></code> and it is the user’s responsibility to
ensure that this is set so that each rank has an individual GPU, via
<code class="docutils literal notranslate"><span class="pre">torch.cuda.set_device()</span></code>.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Object collectives have a number of serious performance and scalability
limitations.  See <span class="xref std std-ref">object_collectives</span> for details.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><a class="reference internal" href="#torchcomms.objcol.recv_object_list" title="torchcomms.objcol.recv_object_list"><code class="xref py py-func docutils literal notranslate"><span class="pre">recv_object_list()</span></code></a> uses <code class="docutils literal notranslate"><span class="pre">pickle</span></code> module implicitly, which
is known to be insecure. It is possible to construct malicious pickle
data which will execute arbitrary code during unpickling. Only call this
function with data you trust.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Calling <a class="reference internal" href="#torchcomms.objcol.recv_object_list" title="torchcomms.objcol.recv_object_list"><code class="xref py py-func docutils literal notranslate"><span class="pre">recv_object_list()</span></code></a> with GPU tensors is not well supported
and inefficient as it incurs GPU -&gt; CPU transfer since tensors would be
pickled. Please consider using <code class="xref py py-func docutils literal notranslate"><span class="pre">recv()</span></code> instead.</p>
</div>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP(&quot;need comm init&quot;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Note: comm initialization omitted on each rank.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchcomms</span><span class="w"> </span><span class="kn">import</span> <span class="n">objcol</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Assumes backend is not NCCL</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">if</span> <span class="n">comm</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="c1"># Assumes world_size of 2.</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">objects</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;foo&quot;</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="p">{</span><span class="mi">1</span><span class="p">:</span> <span class="mi">2</span><span class="p">}]</span> <span class="c1"># any picklable object</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">objcol</span><span class="o">.</span><span class="n">send_object_list</span><span class="p">(</span><span class="n">comm</span><span class="p">,</span> <span class="n">objects</span><span class="p">,</span> <span class="n">dst</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">else</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">objects</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">objcol</span><span class="o">.</span><span class="n">recv_object_list</span><span class="p">(</span><span class="n">comm</span><span class="p">,</span> <span class="n">objects</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">objects</span>
<span class="go">[&#39;foo&#39;, 12, {1: 2}]</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torchcomms.objcol.broadcast_object_list">
<span class="sig-prename descclassname"><span class="pre">torchcomms.objcol.</span></span><span class="sig-name descname"><span class="pre">broadcast_object_list</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">comm</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">TorchComm</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">object_list</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">root</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">timedelta</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights_only</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/torchcomms/objcol.html#broadcast_object_list"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchcomms.objcol.broadcast_object_list" title="Link to this definition">#</a></dt>
<dd><p>Broadcasts picklable objects in <code class="docutils literal notranslate"><span class="pre">object_list</span></code> to the whole comm.</p>
<p>Similar to <code class="xref py py-func docutils literal notranslate"><span class="pre">broadcast()</span></code>, but Python objects can be passed in.
Note that all objects in <code class="docutils literal notranslate"><span class="pre">object_list</span></code> must be picklable in order to be
broadcasted.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>comm</strong> – The comm to work on.</p></li>
<li><p><strong>object_list</strong> (<em>List</em><em>[</em><em>object</em><em>]</em>) – List of input objects to broadcast.
Each object must be picklable. Only objects on the <code class="docutils literal notranslate"><span class="pre">src</span></code> rank will
be broadcast, but each rank must provide lists of equal sizes.</p></li>
<li><p><strong>root</strong> (<em>int</em>) – Source rank from which to broadcast <code class="docutils literal notranslate"><span class="pre">object_list</span></code>.</p></li>
<li><p><strong>timeout</strong> – (timedelta, optional): Timeout for collective operations. If
<code class="docutils literal notranslate"><span class="pre">None</span></code>, will use the default timeout for the backend.</p></li>
<li><p><strong>weights_only</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, only safe objects such as
weights are allowed to be deserialized.
<a class="reference external" href="https://docs.pytorch.org/docs/stable/notes/serialization.html#weights-only">https://docs.pytorch.org/docs/stable/notes/serialization.html#weights-only</a></p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="docutils literal notranslate"><span class="pre">None</span></code>. If rank is part of the comm, <code class="docutils literal notranslate"><span class="pre">object_list</span></code> will contain the
broadcasted objects from <code class="docutils literal notranslate"><span class="pre">src</span></code> rank.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For NCCL-based comms, internal tensor representations
of objects must be moved to the GPU device before communication takes
place. In this case, the device used is given by
<code class="docutils literal notranslate"><span class="pre">torch.cuda.current_device()</span></code> and it is the user’s responsibility to
ensure that this is set so that each rank has an individual GPU, via
<code class="docutils literal notranslate"><span class="pre">torch.cuda.set_device()</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Note that this API differs slightly from the <code class="xref py py-func docutils literal notranslate"><span class="pre">broadcast()</span></code>
collective since it does not provide an <code class="docutils literal notranslate"><span class="pre">async_op</span></code> handle and thus
will be a blocking call.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Object collectives have a number of serious performance and scalability
limitations.  See <span class="xref std std-ref">object_collectives</span> for details.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><a class="reference internal" href="#torchcomms.objcol.broadcast_object_list" title="torchcomms.objcol.broadcast_object_list"><code class="xref py py-func docutils literal notranslate"><span class="pre">broadcast_object_list()</span></code></a> uses <code class="docutils literal notranslate"><span class="pre">pickle</span></code> module implicitly, which
is known to be insecure. It is possible to construct malicious pickle
data which will execute arbitrary code during unpickling. Only call this
function with data you trust.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Calling <a class="reference internal" href="#torchcomms.objcol.broadcast_object_list" title="torchcomms.objcol.broadcast_object_list"><code class="xref py py-func docutils literal notranslate"><span class="pre">broadcast_object_list()</span></code></a> with GPU tensors is not well supported
and inefficient as it incurs GPU -&gt; CPU transfer since tensors would be
pickled. Please consider using <code class="xref py py-func docutils literal notranslate"><span class="pre">broadcast()</span></code> instead.</p>
</div>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP(&quot;need comm init&quot;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Note: comm initialization omitted on each rank.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchcomms</span><span class="w"> </span><span class="kn">import</span> <span class="n">objcol</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">if</span> <span class="n">comm</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="c1"># Assumes world_size of 3.</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">objects</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;foo&quot;</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="p">{</span><span class="mi">1</span><span class="p">:</span> <span class="mi">2</span><span class="p">}]</span> <span class="c1"># any picklable object</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">else</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">objects</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Assumes backend is not NCCL</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">objcol</span><span class="o">.</span><span class="n">broadcast_object_list</span><span class="p">(</span><span class="n">comm</span><span class="p">,</span> <span class="n">objects</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">objects</span>
<span class="go">[&#39;foo&#39;, 12, {1: 2}]</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torchcomms.objcol.scatter_object_list">
<span class="sig-prename descclassname"><span class="pre">torchcomms.objcol.</span></span><span class="sig-name descname"><span class="pre">scatter_object_list</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">comm</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torchcomms.TorchComm" title="torchcomms.TorchComm"><span class="pre">TorchComm</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">root</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scatter_object_output_list</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scatter_object_input_list</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">timedelta</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights_only</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/torchcomms/objcol.html#scatter_object_list"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchcomms.objcol.scatter_object_list" title="Link to this definition">#</a></dt>
<dd><p>Scatters picklable objects in <code class="docutils literal notranslate"><span class="pre">scatter_object_input_list</span></code> to the whole comm.</p>
<p>Similar to <code class="xref py py-func docutils literal notranslate"><span class="pre">scatter()</span></code>, but Python objects can be passed in. On
each rank, the scattered object will be stored as the first element of
<code class="docutils literal notranslate"><span class="pre">scatter_object_output_list</span></code>. Note that all objects in
<code class="docutils literal notranslate"><span class="pre">scatter_object_input_list</span></code> must be picklable in order to be scattered.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>comm</strong> – The comm to work on.</p></li>
<li><p><strong>scatter_object_output_list</strong> (<em>List</em><em>[</em><em>object</em><em>]</em>) – Non-empty list whose first
element will store the object scattered to this rank.</p></li>
<li><p><strong>scatter_object_input_list</strong> (<em>List</em><em>[</em><em>object</em><em>]</em><em>, </em><em>optional</em>) – List of input objects to scatter.
Each object must be picklable. Only objects on the <code class="docutils literal notranslate"><span class="pre">root</span></code> rank will
be scattered, and the argument can be <code class="docutils literal notranslate"><span class="pre">None</span></code> for non-root ranks.</p></li>
<li><p><strong>root</strong> (<em>int</em>) – Source rank from which to scatter <code class="docutils literal notranslate"><span class="pre">scatter_object_input_list</span></code>.</p></li>
<li><p><strong>timeout</strong> – (timedelta, optional): Timeout for collective operations. If
<code class="docutils literal notranslate"><span class="pre">None</span></code>, will use the default timeout for the backend.</p></li>
<li><p><strong>weights_only</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, only safe objects such as
weights are allowed to be deserialized.
<a class="reference external" href="https://docs.pytorch.org/docs/stable/notes/serialization.html#weights-only">https://docs.pytorch.org/docs/stable/notes/serialization.html#weights-only</a></p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="docutils literal notranslate"><span class="pre">None</span></code>. If rank is part of the comm, <code class="docutils literal notranslate"><span class="pre">scatter_object_output_list</span></code>
will have its first element set to the scattered object for this rank.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Note that this API differs slightly from the scatter collective
since it does not provide an <code class="docutils literal notranslate"><span class="pre">async_op</span></code> handle and thus will be a
blocking call.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Object collectives have a number of serious performance and scalability
limitations.  See <span class="xref std std-ref">object_collectives</span> for details.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><a class="reference internal" href="#torchcomms.objcol.scatter_object_list" title="torchcomms.objcol.scatter_object_list"><code class="xref py py-func docutils literal notranslate"><span class="pre">scatter_object_list()</span></code></a> uses <code class="docutils literal notranslate"><span class="pre">pickle</span></code> module implicitly, which
is known to be insecure. It is possible to construct malicious pickle
data which will execute arbitrary code during unpickling. Only call this
function with data you trust.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Calling <a class="reference internal" href="#torchcomms.objcol.scatter_object_list" title="torchcomms.objcol.scatter_object_list"><code class="xref py py-func docutils literal notranslate"><span class="pre">scatter_object_list()</span></code></a> with GPU tensors is not well supported
and inefficient as it incurs GPU -&gt; CPU transfer since tensors would be
pickled. Please consider using <code class="xref py py-func docutils literal notranslate"><span class="pre">scatter()</span></code> instead.</p>
</div>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP(&quot;need comm init&quot;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Note: comm initialization omitted on each rank.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchcomms</span><span class="w"> </span><span class="kn">import</span> <span class="n">objcol</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">if</span> <span class="n">comm</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="c1"># Assumes world_size of 3.</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">objects</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;foo&quot;</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="p">{</span><span class="mi">1</span><span class="p">:</span> <span class="mi">2</span><span class="p">}]</span> <span class="c1"># any picklable object</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">else</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="c1"># Can be any list on non-root ranks, elements are not used.</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">objects</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_list</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">objcol</span><span class="o">.</span><span class="n">scatter_object_list</span><span class="p">(</span><span class="n">comm</span><span class="p">,</span> <span class="n">output_list</span><span class="p">,</span> <span class="n">objects</span><span class="p">,</span> <span class="n">root</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Rank i gets objects[i]. For example, on rank 2:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_list</span>
<span class="go">[{1: 2}]</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
    <a class="left-prev"
       href="getting_started.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Getting Started</p>
      </div>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="getting_started.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Getting Started</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.new_comm"><code class="docutils literal notranslate"><span class="pre">new_comm()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchComm"><code class="docutils literal notranslate"><span class="pre">TorchComm</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchComm.__init__"><code class="docutils literal notranslate"><span class="pre">TorchComm.__init__()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchComm.all_gather"><code class="docutils literal notranslate"><span class="pre">TorchComm.all_gather()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchComm.all_gather_single"><code class="docutils literal notranslate"><span class="pre">TorchComm.all_gather_single()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchComm.all_gather_v"><code class="docutils literal notranslate"><span class="pre">TorchComm.all_gather_v()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchComm.all_reduce"><code class="docutils literal notranslate"><span class="pre">TorchComm.all_reduce()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchComm.all_to_all"><code class="docutils literal notranslate"><span class="pre">TorchComm.all_to_all()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchComm.all_to_all_single"><code class="docutils literal notranslate"><span class="pre">TorchComm.all_to_all_single()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchComm.all_to_all_v_single"><code class="docutils literal notranslate"><span class="pre">TorchComm.all_to_all_v_single()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchComm.barrier"><code class="docutils literal notranslate"><span class="pre">TorchComm.barrier()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchComm.batch_op_create"><code class="docutils literal notranslate"><span class="pre">TorchComm.batch_op_create()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchComm.broadcast"><code class="docutils literal notranslate"><span class="pre">TorchComm.broadcast()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchComm.finalize"><code class="docutils literal notranslate"><span class="pre">TorchComm.finalize()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchComm.gather"><code class="docutils literal notranslate"><span class="pre">TorchComm.gather()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchComm.get_backend"><code class="docutils literal notranslate"><span class="pre">TorchComm.get_backend()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchComm.get_device"><code class="docutils literal notranslate"><span class="pre">TorchComm.get_device()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchComm.get_name"><code class="docutils literal notranslate"><span class="pre">TorchComm.get_name()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchComm.get_options"><code class="docutils literal notranslate"><span class="pre">TorchComm.get_options()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchComm.get_rank"><code class="docutils literal notranslate"><span class="pre">TorchComm.get_rank()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchComm.get_size"><code class="docutils literal notranslate"><span class="pre">TorchComm.get_size()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchComm.mem_allocator"><code class="docutils literal notranslate"><span class="pre">TorchComm.mem_allocator</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchComm.new_window"><code class="docutils literal notranslate"><span class="pre">TorchComm.new_window()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchComm.recv"><code class="docutils literal notranslate"><span class="pre">TorchComm.recv()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchComm.reduce"><code class="docutils literal notranslate"><span class="pre">TorchComm.reduce()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchComm.reduce_scatter"><code class="docutils literal notranslate"><span class="pre">TorchComm.reduce_scatter()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchComm.reduce_scatter_single"><code class="docutils literal notranslate"><span class="pre">TorchComm.reduce_scatter_single()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchComm.reduce_scatter_v"><code class="docutils literal notranslate"><span class="pre">TorchComm.reduce_scatter_v()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchComm.scatter"><code class="docutils literal notranslate"><span class="pre">TorchComm.scatter()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchComm.send"><code class="docutils literal notranslate"><span class="pre">TorchComm.send()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchComm.split"><code class="docutils literal notranslate"><span class="pre">TorchComm.split()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchComm.unsafe_get_backend"><code class="docutils literal notranslate"><span class="pre">TorchComm.unsafe_get_backend()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.ReduceOp"><code class="docutils literal notranslate"><span class="pre">ReduceOp</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.ReduceOp.AVG"><code class="docutils literal notranslate"><span class="pre">ReduceOp.AVG</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.ReduceOp.BAND"><code class="docutils literal notranslate"><span class="pre">ReduceOp.BAND</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.ReduceOp.BOR"><code class="docutils literal notranslate"><span class="pre">ReduceOp.BOR</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.ReduceOp.BXOR"><code class="docutils literal notranslate"><span class="pre">ReduceOp.BXOR</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.ReduceOp.MAX"><code class="docutils literal notranslate"><span class="pre">ReduceOp.MAX</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.ReduceOp.MIN"><code class="docutils literal notranslate"><span class="pre">ReduceOp.MIN</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.ReduceOp.PREMUL_SUM"><code class="docutils literal notranslate"><span class="pre">ReduceOp.PREMUL_SUM()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.ReduceOp.PRODUCT"><code class="docutils literal notranslate"><span class="pre">ReduceOp.PRODUCT</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.ReduceOp.SUM"><code class="docutils literal notranslate"><span class="pre">ReduceOp.SUM</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.ReduceOp.__init__"><code class="docutils literal notranslate"><span class="pre">ReduceOp.__init__()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.ReduceOp.type"><code class="docutils literal notranslate"><span class="pre">ReduceOp.type</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchWork"><code class="docutils literal notranslate"><span class="pre">TorchWork</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchWork.__init__"><code class="docutils literal notranslate"><span class="pre">TorchWork.__init__()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchWork.is_completed"><code class="docutils literal notranslate"><span class="pre">TorchWork.is_completed()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchWork.wait"><code class="docutils literal notranslate"><span class="pre">TorchWork.wait()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.BatchP2POptions"><code class="docutils literal notranslate"><span class="pre">BatchP2POptions</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.BatchP2POptions.__init__"><code class="docutils literal notranslate"><span class="pre">BatchP2POptions.__init__()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.BatchP2POptions.hints"><code class="docutils literal notranslate"><span class="pre">BatchP2POptions.hints</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.BatchP2POptions.timeout"><code class="docutils literal notranslate"><span class="pre">BatchP2POptions.timeout</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.BatchSendRecv"><code class="docutils literal notranslate"><span class="pre">BatchSendRecv</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.BatchSendRecv.__init__"><code class="docutils literal notranslate"><span class="pre">BatchSendRecv.__init__()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.BatchSendRecv.issue"><code class="docutils literal notranslate"><span class="pre">BatchSendRecv.issue()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.BatchSendRecv.ops"><code class="docutils literal notranslate"><span class="pre">BatchSendRecv.ops</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.BatchSendRecv.recv"><code class="docutils literal notranslate"><span class="pre">BatchSendRecv.recv()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.BatchSendRecv.send"><code class="docutils literal notranslate"><span class="pre">BatchSendRecv.send()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.P2POp"><code class="docutils literal notranslate"><span class="pre">P2POp</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.P2POp.__init__"><code class="docutils literal notranslate"><span class="pre">P2POp.__init__()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.P2POp.peer"><code class="docutils literal notranslate"><span class="pre">P2POp.peer</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.P2POp.tensor"><code class="docutils literal notranslate"><span class="pre">P2POp.tensor</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.P2POp.type"><code class="docutils literal notranslate"><span class="pre">P2POp.type</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.CommOptions"><code class="docutils literal notranslate"><span class="pre">CommOptions</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.CommOptions.__init__"><code class="docutils literal notranslate"><span class="pre">CommOptions.__init__()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.CommOptions.abort_process_on_timeout_or_error"><code class="docutils literal notranslate"><span class="pre">CommOptions.abort_process_on_timeout_or_error</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.CommOptions.hints"><code class="docutils literal notranslate"><span class="pre">CommOptions.hints</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.CommOptions.store"><code class="docutils literal notranslate"><span class="pre">CommOptions.store</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.CommOptions.timeout"><code class="docutils literal notranslate"><span class="pre">CommOptions.timeout</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchCommWindow"><code class="docutils literal notranslate"><span class="pre">TorchCommWindow</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchCommWindow.__init__"><code class="docutils literal notranslate"><span class="pre">TorchCommWindow.__init__()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchCommWindow.get_attr"><code class="docutils literal notranslate"><span class="pre">TorchCommWindow.get_attr()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchCommWindow.get_size"><code class="docutils literal notranslate"><span class="pre">TorchCommWindow.get_size()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchCommWindow.map_remote_tensor"><code class="docutils literal notranslate"><span class="pre">TorchCommWindow.map_remote_tensor()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchCommWindow.put"><code class="docutils literal notranslate"><span class="pre">TorchCommWindow.put()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchCommWindow.signal"><code class="docutils literal notranslate"><span class="pre">TorchCommWindow.signal()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchCommWindow.tensor_deregister"><code class="docutils literal notranslate"><span class="pre">TorchCommWindow.tensor_deregister()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchCommWindow.tensor_register"><code class="docutils literal notranslate"><span class="pre">TorchCommWindow.tensor_register()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.TorchCommWindow.wait_signal"><code class="docutils literal notranslate"><span class="pre">TorchCommWindow.wait_signal()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.device_mesh.init_device_mesh"><code class="docutils literal notranslate"><span class="pre">init_device_mesh()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.objcol.get_serialization"><code class="docutils literal notranslate"><span class="pre">get_serialization()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.objcol.all_gather_object"><code class="docutils literal notranslate"><span class="pre">all_gather_object()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.objcol.gather_object"><code class="docutils literal notranslate"><span class="pre">gather_object()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.objcol.send_object_list"><code class="docutils literal notranslate"><span class="pre">send_object_list()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.objcol.recv_object_list"><code class="docutils literal notranslate"><span class="pre">recv_object_list()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.objcol.broadcast_object_list"><code class="docutils literal notranslate"><span class="pre">broadcast_object_list()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torchcomms.objcol.scatter_object_list"><code class="docutils literal notranslate"><span class="pre">scatter_object_list()</span></code></a></li>
</ul>
  </nav></div>
    




</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  


<style>
.site-footer {
    padding: 20px 40px;
    height: 60px !important;
}

@media screen and (min-width: 768px) {
    .site-footer {
        padding: 20px 40px;
    }
}

.site-footer .privacy-policy {
    border-top: none;
    margin-top: 0px;
}

.site-footer .privacy-policy .copyright {
    padding-top: 0;
}
</style>


<footer class="site-footer">

    <div class="privacy-policy">
      <div class="copyright">
      
        <p>
           Copyright © 2025 Meta Platforms, Inc
        </p>
        
      </div>
    </div>


  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2025, torchcomms Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "API Reference",
       "headline": "API Reference",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/api.html",
       "articleBody": "API Reference# torchcomms.new_comm(backend: str, device: torch.device, name: str, abort_process_on_timeout_or_error: bool | None = None, timeout: datetime.timedelta | None = None, high_priority_stream: bool | None = None, store: torch.distributed.distributed_c10d.Store | None = None, hints: collections.abc.Mapping[str, str] | None = None) \u2192 torchcomms.TorchComm# Create a new communicator. This requires all ranks that will be part of the commmunicator call this function simultaneously. Ranks and world size will be derived from environment variables set by launchers such as torchrun (i.e. RANK, WORLD_SIZE). Backends typically use a store to initialize which can either be provided or automatically instantiated from environment variables such as MASTER_ADDR and MASTER_PORT. Backends are not required to use the store to initialize if more performant options are available. Subcommunicators can be instantiated by using the split method. Parameters: backend (str) \u2013 The backend to use for the communicator. device (torch.device) \u2013 The device to use for the communicator. name (str) \u2013 The name of the communicator. This must be unique within the process. abort_process_on_timeout_or_error (bool) \u2013 Whether to abort process on timeout or error. timeout (timedelta) \u2013 Timeout for initialization. high_priority_stream (bool) \u2013 Whether to use high priority stream. store (torch.distributed.Store) \u2013 Store used to initialize the communicator between processes. hints (dict) \u2013 Dictionary of string hints for backend-specific options. class torchcomms.TorchComm# Bases: pybind11_object __init__(*args, **kwargs)# all_gather(self: torchcomms.TorchComm, tensor_list: collections.abc.Sequence[torch.Tensor], tensor: torch.Tensor, async_op: bool, hints: collections.abc.Mapping[str, str] | None = None, timeout: datetime.timedelta | None = None) \u2192 torchcomms.TorchWork# Gather a tensor from all ranks in the communicator. Output will be available on all ranks. Parameters: tensor_list \u2013 the list of tensors to gather into tensor \u2013 the input tensor to share async_op \u2013 whether to perform the operation asynchronously hints \u2013 dictionary of string hints for backend-specific options timeout \u2013 timeout for the operation all_gather_single(self: torchcomms.TorchComm, output: torch.Tensor, input: torch.Tensor, async_op: bool, hints: collections.abc.Mapping[str, str] | None = None, timeout: datetime.timedelta | None = None) \u2192 torchcomms.TorchWork# Gather a single tensor from all ranks in the communicator. The output tensor must be of size (world_size * input.numel()). Parameters: output \u2013 the output tensor to gather into input \u2013 the input tensor to share async_op \u2013 whether to perform the operation asynchronously hints \u2013 dictionary of string hints for backend-specific options timeout \u2013 timeout for the operation all_gather_v(self: torchcomms.TorchComm, tensor_list: collections.abc.Sequence[torch.Tensor], tensor: torch.Tensor, async_op: bool, hints: collections.abc.Mapping[str, str] | None = None, timeout: datetime.timedelta | None = None) \u2192 torchcomms.TorchWork# Gather a tensor from all ranks in the communicator, supporting variable tensor sizes per rank. Output will be available on all ranks. Parameters: tensor_list \u2013 the list of tensors to gather into; the list is the same on all ranks, but tensor sizes may differ between indices. tensor \u2013 the input tensor to share; size may differ per rank. async_op \u2013 whether to perform the operation asynchronously hints \u2013 dictionary of string hints for backend-specific options timeout \u2013 timeout for the operation all_reduce(self: torchcomms.TorchComm, tensor: torch.Tensor, op: torchcomms.ReduceOp, async_op: bool, hints: collections.abc.Mapping[str, str] | None = None, timeout: datetime.timedelta | None = None) \u2192 torchcomms.TorchWork# Reduce a tensor across all ranks in the communicator. Parameters: tensor \u2013 the tensor to all-reduce op \u2013 the reduction operation async_op \u2013 whether to perform the operation asynchronously hints \u2013 dictionary of string hints for backend-specific options timeout \u2013 timeout for the operation all_to_all(self: torchcomms.TorchComm, output_tensor_list: collections.abc.Sequence[torch.Tensor], input_tensor_list: collections.abc.Sequence[torch.Tensor], async_op: bool, hints: collections.abc.Mapping[str, str] | None = None, timeout: datetime.timedelta | None = None) \u2192 torchcomms.TorchWork# Scatter the split list to all ranks. Parameters: output_tensor_list \u2013 Output tensor list. input_tensor_list \u2013 Input tensor list to scatter. async_op \u2013 Whether to perform the operation asynchronously. hints \u2013 Dictionary of string hints for backend-specific options. timeout \u2013 Timeout for the operation. all_to_all_single(self: torchcomms.TorchComm, output: torch.Tensor, input: torch.Tensor, async_op: bool, hints: collections.abc.Mapping[str, str] | None = None, timeout: datetime.timedelta | None = None) \u2192 torchcomms.TorchWork# Split input tensor and then scatter the split list to all ranks. Later the received tensors are concatenated and returned as a single output tensor. The input and output tensor sizes must a multiple of world_size. Parameters: output \u2013 Output tensor. input \u2013 Input tensor to split and scatter. async_op \u2013 Whether to perform the operation asynchronously. hints \u2013 Dictionary of string hints for backend-specific options. timeout \u2013 Timeout for the operation. all_to_all_v_single(self: torchcomms.TorchComm, output: torch.Tensor, input: torch.Tensor, output_split_sizes: collections.abc.Sequence[SupportsInt], input_split_sizes: collections.abc.Sequence[SupportsInt], async_op: bool, hints: collections.abc.Mapping[str, str] | None = None, timeout: datetime.timedelta | None = None) \u2192 torchcomms.TorchWork# All-to-all single tensor operation with variable split sizes. Parameters: output \u2013 Output tensor. input \u2013 Input tensor to split and scatter. output_split_sizes \u2013 List of output split sizes. input_split_sizes \u2013 List of input split sizes. async_op \u2013 Whether to perform the operation asynchronously. hints \u2013 Dictionary of string hints for backend-specific options. timeout \u2013 Timeout for the operation. barrier(self: torchcomms.TorchComm, async_op: bool, hints: collections.abc.Mapping[str, str] | None = None, timeout: datetime.timedelta | None = None) \u2192 torchcomms.TorchWork# Block until all ranks have reached this call. Parameters: async_op \u2013 Whether to perform the operation asynchronously. hints \u2013 Dictionary of string hints for backend-specific options. timeout \u2013 Timeout for the operation. batch_op_create(self: torchcomms.TorchComm) \u2192 torchcomms.BatchSendRecv# Create a batch operation object for batched P2P operations. broadcast(self: torchcomms.TorchComm, tensor: torch.Tensor, root: SupportsInt, async_op: bool, hints: collections.abc.Mapping[str, str] | None = None, timeout: datetime.timedelta | None = None) \u2192 torchcomms.TorchWork# Broadcast tensor to all ranks in the communicator. Parameters: tensor \u2013 the tensor to broadcast if root or receive into if not root root \u2013 the root rank async_op \u2013 whether to perform the operation asynchronously hints \u2013 dictionary of string hints for backend-specific options timeout \u2013 timeout for the operation finalize(self: torchcomms.TorchComm) \u2192 None# Finalize and free all resources. This must be called prior to destruction. gather(self: torchcomms.TorchComm, output_tensor_list: collections.abc.Sequence[torch.Tensor], input_tensor: torch.Tensor, root: SupportsInt, async_op: bool, hints: collections.abc.Mapping[str, str] | None = None, timeout: datetime.timedelta | None = None) \u2192 torchcomms.TorchWork# Gather the input tensor from all ranks to the root. Parameters: output_tensor_list \u2013 Output tensor list. Will be empty on non-root ranks. input_tensor \u2013 Input tensor to gather. root \u2013 The root rank. async_op \u2013 Whether to perform the operation asynchronously. hints \u2013 Dictionary of string hints for backend-specific options. timeout \u2013 Timeout for the operation. get_backend(self: torchcomms.TorchComm) \u2192 str# Get communicator backend name get_device(self: torchcomms.TorchComm) \u2192 torch.device# Get the communicator device get_name(self: torchcomms.TorchComm) \u2192 str# Get the name of the communicator get_options(self: torchcomms.TorchComm) \u2192 torchcomms.CommOptions# Get the communicator options get_rank(self: torchcomms.TorchComm) \u2192 int# Get the rank of this process get_size(self: torchcomms.TorchComm) \u2192 int# Get the world size property mem_allocator# Get the communication-specific memory allocator new_window(self: torchcomms.TorchComm) \u2192 torchcomms.TorchCommWindow# Create a new window object for RMA operations. Returns: An unregistered window object. Call window.tensor_register(tensor) to register a tensor buffer. Return type: TorchCommWindow recv(self: torchcomms.TorchComm, tensor: torch.Tensor, src: SupportsInt, async_op: bool, hints: collections.abc.Mapping[str, str] | None = None, timeout: datetime.timedelta | None = None) \u2192 torchcomms.TorchWork# Receive tensor from source rank. This will not run concurrently with other operations (including send/recv) on the same stream. Parameters: tensor \u2013 the tensor to receive into src \u2013 the source rank async_op \u2013 whether to perform the operation asynchronously hints \u2013 dictionary of string hints for backend-specific options timeout \u2013 timeout for the operation reduce(self: torchcomms.TorchComm, tensor: torch.Tensor, root: SupportsInt, op: torchcomms.ReduceOp, async_op: bool, hints: collections.abc.Mapping[str, str] | None = None, timeout: datetime.timedelta | None = None) \u2192 torchcomms.TorchWork# Reduce a tensor from all ranks to a single rank in the communicator. Output will only be available on the root rank. Parameters: tensor \u2013 the tensor to reduce root \u2013 the root rank op \u2013 the reduction operation async_op \u2013 whether to perform the operation asynchronously hints \u2013 dictionary of string hints for backend-specific options timeout \u2013 timeout for the operation reduce_scatter(self: torchcomms.TorchComm, output: torch.Tensor, input_list: collections.abc.Sequence[torch.Tensor], op: torchcomms.ReduceOp, async_op: bool, hints: collections.abc.Mapping[str, str] | None = None, timeout: datetime.timedelta | None = None) \u2192 torchcomms.TorchWork# Reduce, then scatter a list of tensors to all ranks. Parameters: output \u2013 Output tensor. input_list \u2013 List of tensors to reduce and scatter. op \u2013 Reduction operation. async_op \u2013 Whether to perform the operation asynchronously. hints \u2013 Dictionary of string hints for backend-specific options. timeout \u2013 Timeout for the operation. reduce_scatter_single(self: torchcomms.TorchComm, output: torch.Tensor, input: torch.Tensor, op: torchcomms.ReduceOp, async_op: bool, hints: collections.abc.Mapping[str, str] | None = None, timeout: datetime.timedelta | None = None) \u2192 torchcomms.TorchWork# Reduce, then scatter a single tensor to all ranks. The input tensor must be of size (world_size * output.numel()). Parameters: output \u2013 Output tensor. input \u2013 Input tensor to reduce and scatter. op \u2013 Reduction operation. async_op \u2013 Whether to perform the operation asynchronously. hints \u2013 Dictionary of string hints for backend-specific options. timeout \u2013 Timeout for the operation. reduce_scatter_v(self: torchcomms.TorchComm, output: torch.Tensor, input_list: collections.abc.Sequence[torch.Tensor], op: torchcomms.ReduceOp, async_op: bool, hints: collections.abc.Mapping[str, str] | None = None, timeout: datetime.timedelta | None = None) \u2192 torchcomms.TorchWork# Reduce, then scatter a list of tensors to all ranks, supporting variable tensor sizes per rank. Parameters: output \u2013 Output tensor on each rank; size may differ per rank. input_list \u2013 List of tensors to reduce and scatter; the list is the same on all ranks, but tensor sizes may differ between indices. op \u2013 Reduction operation. async_op \u2013 Whether to perform the operation asynchronously hints \u2013 Dictionary of string hints for backend-specific options. timeout \u2013 Timeout for the operation. scatter(self: torchcomms.TorchComm, output_tensor: torch.Tensor, input_tensor_list: collections.abc.Sequence[torch.Tensor], root: SupportsInt, async_op: bool, hints: collections.abc.Mapping[str, str] | None = None, timeout: datetime.timedelta | None = None) \u2192 torchcomms.TorchWork# Scatter the split list to all ranks from the root. Parameters: output_tensor \u2013 Output tensor. input_tensor_list \u2013 Input tensor list to scatter. root \u2013 The root rank. async_op \u2013 Whether to perform the operation asynchronously. hints \u2013 Dictionary of string hints for backend-specific options. timeout \u2013 Timeout for the operation. send(self: torchcomms.TorchComm, tensor: torch.Tensor, dst: SupportsInt, async_op: bool, hints: collections.abc.Mapping[str, str] | None = None, timeout: datetime.timedelta | None = None) \u2192 torchcomms.TorchWork# Send tensor to destination rank. This will not run concurrently with other operations (including send/recv) on the same stream. Parameters: tensor \u2013 the tensor to send dst \u2013 the destination rank async_op \u2013 whether to perform the operation asynchronously hints \u2013 dictionary of string hints for backend-specific options timeout \u2013 timeout for the operation split(self: torchcomms.TorchComm, ranks: collections.abc.Sequence[SupportsInt], name: str, hints: collections.abc.Mapping[str, str] | None = None, timeout: datetime.timedelta | None = None) \u2192 torchcomms.TorchComm# Split communicator into a subgroup. Parameters: ranks \u2013 List of ranks to include in the new subgroup. If the list is empty, None will be returned. If the list is non-empty but does not include the current rank, an exception will be thrown. name \u2013 Name for the new communicator. hints \u2013 Dictionary of string hints for backend-specific options. timeout \u2013 Timeout for the operation. Returns: A new communicator for the subgroup, or None if the ranks list is empty. Raises: RuntimeError if the ranks list is non-empty and the current rank is not included. unsafe_get_backend(self: torchcomms.TorchComm) \u2192 torchcomms.TorchCommBackend# Get communicator backend implementation. WARNING: This is intended as an escape hatch for experimentation and development. Direct backend access provides no backwards compatibility guarantees. Users depending on unsafe_get_backend should expect their code to break as interfaces change. class torchcomms.ReduceOp# Bases: pybind11_object Operation to perform during reduction. AVG = \u003ctorchcomms.ReduceOp object\u003e# BAND = \u003ctorchcomms.ReduceOp object\u003e# BOR = \u003ctorchcomms.ReduceOp object\u003e# BXOR = \u003ctorchcomms.ReduceOp object\u003e# MAX = \u003ctorchcomms.ReduceOp object\u003e# MIN = \u003ctorchcomms.ReduceOp object\u003e# static PREMUL_SUM(factor: torch.Tensor | SupportsFloat) \u2192 torchcomms.ReduceOp# PRODUCT = \u003ctorchcomms.ReduceOp object\u003e# SUM = \u003ctorchcomms.ReduceOp object\u003e# __init__(self: torchcomms.ReduceOp, opType: torchcomms.RedOpType) \u2192 None# Create default ReduceOp property type# Get the type of the operation class torchcomms.TorchWork# Bases: pybind11_object TorchWork allows you to track whether an asynchronous operation has completed. When async_op=True, the operation is enqueued on a background stream and a TorchWork object is returned. This work object must be waited on before using the output tensor. This is intended to make it easier to write efficient code that can overlap communication with computation. Example usage: tensor = ... # run all_reduce on a background stream and return a TorchWork object work = torchcomms.all_reduce(tensor, ReduceOp.SUM, async_op=True) # Schedule some other work on the current stream a = b * 2 # block the current stream until the all_reduce is complete work.wait() # safely use the tensor after the collective completes tensor.sum() # block CPU until stream is complete torch.accelerator.current_stream().synchronize() __init__(*args, **kwargs)# is_completed(self: torchcomms.TorchWork) \u2192 bool# Check if the work is completed wait(self: torchcomms.TorchWork) \u2192 None# Block the current stream until the work is completed. See https://docs.pytorch.org/docs/stable/notes/cuda.html#cuda-streams for more details. class torchcomms.BatchP2POptions# Bases: pybind11_object Options for batched P2P operations. __init__(self: torchcomms.BatchP2POptions) \u2192 None# Create default BatchP2POptions property hints# Hints dictionary property timeout# Timeout class torchcomms.BatchSendRecv# Bases: pybind11_object BatchSendRecv allows you to run multiple send/recv operations concurrently unlike the standard send/recv APIs which only allow you to have one inflight at a time. __init__(*args, **kwargs)# issue(self: torchcomms.BatchSendRecv, async_op: bool, options: torchcomms.BatchP2POptions = \u003ctorchcomms.BatchP2POptions object at 0x7f1498037ef0\u003e) \u2192 torchcomms.TorchWork# Issues the batched operations property ops# List of P2P operations recv(self: torchcomms.BatchSendRecv, tensor: torch.Tensor, src: SupportsInt) \u2192 None# Add recv operation to batch. Must be paired with a corresponding send operation on a different rank. Parameters: tensor \u2013 the tensor to receive into src \u2013 the source rank send(self: torchcomms.BatchSendRecv, tensor: torch.Tensor, dst: SupportsInt) \u2192 None# Add send operation to batch. Must be paired with a corresponding recv operation on a different rank. Parameters: tensor \u2013 the tensor to send dst \u2013 the destination rank class torchcomms.P2POp# Bases: pybind11_object Represents a peer to peer operation as part of a batch. __init__(self: torchcomms.P2POp, type: torch::comms::BatchSendRecv::P2POp::OpType, tensor: torch.Tensor, peer: typing.SupportsInt) \u2192 None# Create P2POp. Parameters: type \u2013 the type of the operations i.e. send/recv tensor \u2013 the tensor to operate on peer \u2013 the rank of the peer property peer# Peer rank property tensor# Tensor property type# Operation type class torchcomms.CommOptions# Bases: pybind11_object Options for communicator creation. __init__(self: torchcomms.CommOptions) \u2192 None# Create default CommOptions property abort_process_on_timeout_or_error# Whether to abort process on timeout or error property hints# Dictionary of string hints for backend-specific options property store# Store for communication between processes property timeout# Timeout for operations (milliseconds) class torchcomms.TorchCommWindow# Bases: pybind11_object __init__(*args, **kwargs)# get_attr(self: torchcomms.TorchCommWindow, peer_rank: SupportsInt) \u2192 torchcomms.TorchCommWindowAttr# get the attribute of the window get_size(self: torchcomms.TorchCommWindow) \u2192 int# Get the size of the window map_remote_tensor(self: torchcomms.TorchCommWindow, rank: SupportsInt) \u2192 torch.Tensor# Get the entire tensor view from the remote rank\u2019s window buffer. This method returns a tensor view of the complete registered buffer from the specified rank\u2019s window. Users can slice the returned tensor as needed. Parameters: rank \u2013 The rank whose window to access. Returns: A tensor view with the same shape as the registered buffer. Example If the registered buffer has shape [100, 512, 128]: - full_tensor = map_remote_tensor(rank=0) returns a tensor with shape [100, 512, 128] You can then slice it: sliced = full_tensor[20:65] put(self: torchcomms.TorchCommWindow, tensor: torch.Tensor, dst_rank: SupportsInt, target_offset_nelems: SupportsInt, async_op: bool, hints: collections.abc.Mapping[str, str] | None = None, timeout: datetime.timedelta | None = None) \u2192 torchcomms.TorchWork# Put allows you to put a tensor into the previously allocated remote window. Parameters: tensor \u2013 the tensor to put dst_rank \u2013 the destination rank target_offset_nelems \u2013 the target offset in number of elements async_op \u2013 if this is true, the operation is asynced and will be enqueued on a background stream and a TorchWork object is returned. Example usage: tensor = ... # create a window window = torchcomms.create_window(window_size, cpu_buf) # put a tensor into the window work = window.put(tensor, dst_rank, target_offset_nelems, async_op=True) work.wait() # on the remote side, get the tensor from the window after waiting on the remote signal tensor = window.map_remote_tensor(rank) # safely use the tensor after the collective completes tensor.sum() signal(self: torchcomms.TorchCommWindow, peer_rank: SupportsInt, async_op: bool, hints: collections.abc.Mapping[str, str] | None = None, timeout: datetime.timedelta | None = None) \u2192 torchcomms.TorchWork# Atomic signal to notify remote peer of a change in state. Parameters: peer_rank \u2013 the rank of the remote peer to signal. async_op \u2013 if this is true, the operation is asynced. tensor_deregister(self: torchcomms.TorchCommWindow) \u2192 None# Deregister the window and free all associated resources. This is a collective operation that includes internal barriers to ensure: 1. All ranks have finished using the window before deregistration 2. All ranks have completed deregistration before proceeding tensor_register(self: torchcomms.TorchCommWindow, tensor: torch.Tensor) \u2192 None# Register a tensor buffer to a window for RMA operations. Parameters: tensor \u2013 the contiguous tensor buffer to register as a window wait_signal(self: torchcomms.TorchCommWindow, peer_rank: SupportsInt, async_op: bool, hints: collections.abc.Mapping[str, str] | None = None, timeout: datetime.timedelta | None = None) \u2192 torchcomms.TorchWork# wait for a signal from remote peer torchcomms.device_mesh.init_device_mesh(mesh_dim_comms: tuple[TorchComm, ...], mesh_dim_names: tuple[str, ...], _global_comm: TorchComm | None = None) \u2192 DeviceMesh[source]# Initializes a DeviceMesh from the list of provided TorchComm instances. See DeviceMesh for more details. torchcomms.objcol.get_serialization() \u2192 _Serialization[source]# Returns a cached serialization object with serialize and deserialize methods. torchcomms.objcol.all_gather_object(comm: TorchComm, object_list: list[Any], obj: object, timeout: timedelta | None = None, weights_only: bool = True) \u2192 None[source]# Gathers picklable objects from the whole comm into a list. Similar to all_gather(), but Python objects can be passed in. Note that the object must be picklable in order to be gathered. Parameters: comm \u2013 The comm to work on. object_list (list[object]) \u2013 Output list. It should be correctly sized as the size of the comm for this collective and will contain the output. obj (object) \u2013 Pickable Python object to be broadcast from current process. timeout \u2013 (timedelta, optional): Timeout for collective operations. If None, will use the default timeout for the backend. weights_only (bool, optional) \u2013 If True, only safe objects such as weights are allowed to be deserialized. https://docs.pytorch.org/docs/stable/notes/serialization.html#weights-only Returns: None. If the calling rank is part of this comm, the output of the collective will be populated into the input object_list. If the calling rank is not part of the comm, the passed in object_list will be unmodified. Note Note that this API differs slightly from the all_gather() collective since it does not provide an async_op handle and thus will be a blocking call. Note For NCCL-based processed comms, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by torch.cuda.current_device() and it is the user\u2019s responsibility to ensure that this is set so that each rank has an individual GPU, via torch.cuda.set_device(). Warning Object collectives have a number of serious performance and scalability limitations. See object_collectives for details. Warning all_gather_object() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust. Warning Calling all_gather_object() with GPU tensors is not well supported and inefficient as it incurs GPU -\u003e CPU transfer since tensors would be pickled. Please consider using all_gather() instead. Example::\u003e\u003e\u003e # xdoctest: +SKIP(\"need comm init\") \u003e\u003e\u003e # Note: comm initialization omitted on each rank. \u003e\u003e\u003e from torchcomms import objcol \u003e\u003e\u003e # Assumes world_size of 3. \u003e\u003e\u003e gather_objects = [\"foo\", 12, {1: 2}] # any picklable object \u003e\u003e\u003e output = [None for _ in gather_objects] \u003e\u003e\u003e objcol.all_gather_object(comm, output, gather_objects[comm.get_rank()]) \u003e\u003e\u003e output [\u0027foo\u0027, 12, {1: 2}] torchcomms.objcol.gather_object(comm: TorchComm, obj: object, root: int, object_gather_list: list[Any] | None = None, timeout: timedelta | None = None, weights_only: bool = True) \u2192 None[source]# Gathers picklable objects from the whole comm in a single process. Similar to gather(), but Python objects can be passed in. Note that the object must be picklable in order to be gathered. Parameters: comm \u2013 The comm to work on. obj (object) \u2013 Input object. Must be picklable. object_gather_list (list[object]) \u2013 Output list. On the root rank, it should be correctly sized as the size of the comm for this collective and will contain the output. Must be None on non-root ranks. (default is None) root (int, optional) \u2013 Destination rank on comm. Invalid to specify both root and root timeout \u2013 (timedelta, optional): Timeout for collective operations. If None, will use the default timeout for the backend. weights_only (bool, optional) \u2013 If True, only safe objects such as weights are allowed to be deserialized. https://docs.pytorch.org/docs/stable/notes/serialization.html#weights-only Returns: None. On the root rank, object_gather_list will contain the output of the collective. Note Note that this API differs slightly from the gather collective since it does not provide an async_op handle and thus will be a blocking call. Note For NCCL-based processed comms, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by torch.cuda.current_device() and it is the user\u2019s responsibility to ensure that this is set so that each rank has an individual GPU, via torch.cuda.set_device(). Warning Object collectives have a number of serious performance and scalability limitations. See object_collectives for details. Warning gather_object() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust. Warning Calling gather_object() with GPU tensors is not well supported and inefficient as it incurs GPU -\u003e CPU transfer since tensors would be pickled. Please consider using gather() instead. Example::\u003e\u003e\u003e # xdoctest: +SKIP(\"need comm init\") \u003e\u003e\u003e # Note: comm initialization omitted on each rank. \u003e\u003e\u003e from torchcomms import objcol \u003e\u003e\u003e # Assumes world_size of 3. \u003e\u003e\u003e gather_objects = [\"foo\", 12, {1: 2}] # any picklable object \u003e\u003e\u003e output = [None for _ in gather_objects] \u003e\u003e\u003e objcol.gather_object( ... comm, ... gather_objects[comm.get_rank()], ... output, ... root=0 ... ) \u003e\u003e\u003e # On rank 0 \u003e\u003e\u003e output [\u0027foo\u0027, 12, {1: 2}] torchcomms.objcol.send_object_list(comm: TorchComm, object_list: list[Any], dst: int, timeout: timedelta | None = None) \u2192 None[source]# Sends picklable objects in object_list synchronously. Similar to send(), but Python objects can be passed in. Note that all objects in object_list must be picklable in order to be sent. Parameters: comm \u2013 The comm to work on. object_list (List[object]) \u2013 List of input objects to sent. Each object must be picklable. Receiver must provide lists of equal sizes. dst (int) \u2013 Destination rank to send object_list to. timeout \u2013 (timedelta, optional): Timeout for collective operations. If None, will use the default timeout for the backend. Returns: None. Note For NCCL-based comms, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by torch.cuda.current_device() and it is the user\u2019s responsibility to ensure that this is set so that each rank has an individual GPU, via torch.cuda.set_device(). Warning Object collectives have a number of serious performance and scalability limitations. See object_collectives for details. Warning send_object_list() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust. Warning Calling send_object_list() with GPU tensors is not well supported and inefficient as it incurs GPU -\u003e CPU transfer since tensors would be pickled. Please consider using send() instead. Example::\u003e\u003e\u003e # xdoctest: +SKIP(\"need comm init\") \u003e\u003e\u003e # Note: comm initialization omitted on each rank. \u003e\u003e\u003e from torchcomms import objcol \u003e\u003e\u003e # Assumes backend is not NCCL \u003e\u003e\u003e if comm.get_rank() == 0: \u003e\u003e\u003e # Assumes world_size of 2. \u003e\u003e\u003e objects = [\"foo\", 12, {1: 2}] # any picklable object \u003e\u003e\u003e objcol.send_object_list(comm, objects, dst=1) \u003e\u003e\u003e else: \u003e\u003e\u003e objects = [None, None, None] \u003e\u003e\u003e objcol.recv_object_list(comm, objects, src=0) \u003e\u003e\u003e objects [\u0027foo\u0027, 12, {1: 2}] torchcomms.objcol.recv_object_list(comm: TorchComm, object_list: list[Any], src: int, timeout: timedelta | None = None, weights_only: bool = True) \u2192 None[source]# Receives picklable objects in object_list synchronously. Similar to recv(), but can receive Python objects. Parameters: comm \u2013 The comm to work on. object_list (List[object]) \u2013 List of objects to receive into. Must provide a list of sizes equal to the size of the list being sent. src (int) \u2013 Source rank from which to recv object_list. timeout \u2013 (timedelta, optional): Timeout for collective operations. If None, will use the default timeout for the backend. weights_only (bool, optional) \u2013 If True, only safe objects such as weights are allowed to be deserialized. https://docs.pytorch.org/docs/stable/notes/serialization.html#weights-only Returns: None Note For NCCL-based comms, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by torch.cuda.current_device() and it is the user\u2019s responsibility to ensure that this is set so that each rank has an individual GPU, via torch.cuda.set_device(). Warning Object collectives have a number of serious performance and scalability limitations. See object_collectives for details. Warning recv_object_list() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust. Warning Calling recv_object_list() with GPU tensors is not well supported and inefficient as it incurs GPU -\u003e CPU transfer since tensors would be pickled. Please consider using recv() instead. Example::\u003e\u003e\u003e # xdoctest: +SKIP(\"need comm init\") \u003e\u003e\u003e # Note: comm initialization omitted on each rank. \u003e\u003e\u003e from torchcomms import objcol \u003e\u003e\u003e # Assumes backend is not NCCL \u003e\u003e\u003e if comm.get_rank() == 0: \u003e\u003e\u003e # Assumes world_size of 2. \u003e\u003e\u003e objects = [\"foo\", 12, {1: 2}] # any picklable object \u003e\u003e\u003e objcol.send_object_list(comm, objects, dst=1) \u003e\u003e\u003e else: \u003e\u003e\u003e objects = [None, None, None] \u003e\u003e\u003e objcol.recv_object_list(comm, objects, src=0) \u003e\u003e\u003e objects [\u0027foo\u0027, 12, {1: 2}] torchcomms.objcol.broadcast_object_list(comm: TorchComm, object_list: list[Any], root: int, timeout: timedelta | None = None, weights_only: bool = True) \u2192 None[source]# Broadcasts picklable objects in object_list to the whole comm. Similar to broadcast(), but Python objects can be passed in. Note that all objects in object_list must be picklable in order to be broadcasted. Parameters: comm \u2013 The comm to work on. object_list (List[object]) \u2013 List of input objects to broadcast. Each object must be picklable. Only objects on the src rank will be broadcast, but each rank must provide lists of equal sizes. root (int) \u2013 Source rank from which to broadcast object_list. timeout \u2013 (timedelta, optional): Timeout for collective operations. If None, will use the default timeout for the backend. weights_only (bool, optional) \u2013 If True, only safe objects such as weights are allowed to be deserialized. https://docs.pytorch.org/docs/stable/notes/serialization.html#weights-only Returns: None. If rank is part of the comm, object_list will contain the broadcasted objects from src rank. Note For NCCL-based comms, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by torch.cuda.current_device() and it is the user\u2019s responsibility to ensure that this is set so that each rank has an individual GPU, via torch.cuda.set_device(). Note Note that this API differs slightly from the broadcast() collective since it does not provide an async_op handle and thus will be a blocking call. Warning Object collectives have a number of serious performance and scalability limitations. See object_collectives for details. Warning broadcast_object_list() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust. Warning Calling broadcast_object_list() with GPU tensors is not well supported and inefficient as it incurs GPU -\u003e CPU transfer since tensors would be pickled. Please consider using broadcast() instead. Example::\u003e\u003e\u003e # xdoctest: +SKIP(\"need comm init\") \u003e\u003e\u003e # Note: comm initialization omitted on each rank. \u003e\u003e\u003e from torchcomms import objcol \u003e\u003e\u003e if comm.get_rank() == 0: \u003e\u003e\u003e # Assumes world_size of 3. \u003e\u003e\u003e objects = [\"foo\", 12, {1: 2}] # any picklable object \u003e\u003e\u003e else: \u003e\u003e\u003e objects = [None, None, None] \u003e\u003e\u003e # Assumes backend is not NCCL \u003e\u003e\u003e objcol.broadcast_object_list(comm, objects, src=0, device=device) \u003e\u003e\u003e objects [\u0027foo\u0027, 12, {1: 2}] torchcomms.objcol.scatter_object_list(comm: TorchComm, root: int, scatter_object_output_list: list[Any], scatter_object_input_list: list[Any] | None = None, timeout: timedelta | None = None, weights_only: bool = True) \u2192 None[source]# Scatters picklable objects in scatter_object_input_list to the whole comm. Similar to scatter(), but Python objects can be passed in. On each rank, the scattered object will be stored as the first element of scatter_object_output_list. Note that all objects in scatter_object_input_list must be picklable in order to be scattered. Parameters: comm \u2013 The comm to work on. scatter_object_output_list (List[object]) \u2013 Non-empty list whose first element will store the object scattered to this rank. scatter_object_input_list (List[object], optional) \u2013 List of input objects to scatter. Each object must be picklable. Only objects on the root rank will be scattered, and the argument can be None for non-root ranks. root (int) \u2013 Source rank from which to scatter scatter_object_input_list. timeout \u2013 (timedelta, optional): Timeout for collective operations. If None, will use the default timeout for the backend. weights_only (bool, optional) \u2013 If True, only safe objects such as weights are allowed to be deserialized. https://docs.pytorch.org/docs/stable/notes/serialization.html#weights-only Returns: None. If rank is part of the comm, scatter_object_output_list will have its first element set to the scattered object for this rank. Note Note that this API differs slightly from the scatter collective since it does not provide an async_op handle and thus will be a blocking call. Warning Object collectives have a number of serious performance and scalability limitations. See object_collectives for details. Warning scatter_object_list() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust. Warning Calling scatter_object_list() with GPU tensors is not well supported and inefficient as it incurs GPU -\u003e CPU transfer since tensors would be pickled. Please consider using scatter() instead. Example::\u003e\u003e\u003e # xdoctest: +SKIP(\"need comm init\") \u003e\u003e\u003e # Note: comm initialization omitted on each rank. \u003e\u003e\u003e from torchcomms import objcol \u003e\u003e\u003e if comm.get_rank() == 0: \u003e\u003e\u003e # Assumes world_size of 3. \u003e\u003e\u003e objects = [\"foo\", 12, {1: 2}] # any picklable object \u003e\u003e\u003e else: \u003e\u003e\u003e # Can be any list on non-root ranks, elements are not used. \u003e\u003e\u003e objects = [None, None, None] \u003e\u003e\u003e output_list = [None] \u003e\u003e\u003e objcol.scatter_object_list(comm, output_list, objects, root=0) \u003e\u003e\u003e # Rank i gets objects[i]. For example, on rank 2: \u003e\u003e\u003e output_list [{1: 2}]",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "https://pytorch.org/docs/stable/_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/api.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>